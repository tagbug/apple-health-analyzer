"""Data cleaning and preprocessing module.

Provides data deduplication, merging, quality validation, and other functions.
"""

from collections import defaultdict
from datetime import datetime, timedelta
from typing import Any, cast

import pandas as pd
from pydantic import BaseModel, Field

from src.core.data_models import HealthRecord
from src.utils.logger import get_logger

logger = get_logger(__name__)


class RecordRowData(BaseModel):
  """DataFrame è¡Œæ•°æ®çš„ç±»å‹å®‰å…¨åŒ…è£…

  ç”¨äºåœ¨ DataFrame å’Œ HealthRecord å¯¹è±¡ä¹‹é—´è¿›è¡Œç±»å‹å®‰å…¨çš„è½¬æ¢ã€‚
  """

  type: str
  source_name: str
  start_date: datetime
  end_date: datetime | None
  creation_date: datetime
  source_version: str = "1.0"
  device: str = "Unknown"
  unit: str | None = None
  value: float | None = None
  metadata: dict[str, Any] | None = None

  @classmethod
  def from_series(cls, row: pd.Series, record_type: str) -> "RecordRowData":
    """ä» pandas Series åˆ›å»ºç±»å‹å®‰å…¨çš„æ•°æ®å¯¹è±¡"""
    # æ‰‹åŠ¨æå–å¹¶è½¬æ¢å­—æ®µï¼Œé¿å… pandas ç±»å‹æ¨æ–­é—®é¢˜
    start_date_val = row["start_date"]
    if isinstance(start_date_val, str):
      start_date = pd.to_datetime(start_date_val).to_pydatetime()
    elif isinstance(start_date_val, pd.Timestamp):
      start_date = start_date_val.to_pydatetime()
    else:
      # ä½¿ç”¨ cast æ˜ç¡®å‘Šè¯‰ç±»å‹æ£€æŸ¥å™¨è¿™æ˜¯ datetime
      start_date = cast(datetime, start_date_val)

    end_date_val = row.get("end_date")
    if end_date_val is not None and str(end_date_val).lower() not in (
      "",
      "nan",
      "none",
    ):
      if isinstance(end_date_val, str):
        end_date = pd.to_datetime(end_date_val).to_pydatetime()
      elif isinstance(end_date_val, pd.Timestamp):
        end_date = end_date_val.to_pydatetime()
      else:
        end_date = cast(datetime, end_date_val)
    else:
      end_date = None

    creation_date_val = row["creation_date"]
    if isinstance(creation_date_val, str):
      creation_date = pd.to_datetime(creation_date_val).to_pydatetime()
    elif isinstance(creation_date_val, pd.Timestamp):
      creation_date = creation_date_val.to_pydatetime()
    else:
      # ä½¿ç”¨ cast æ˜ç¡®å‘Šè¯‰ç±»å‹æ£€æŸ¥å™¨è¿™æ˜¯ datetime
      creation_date = cast(datetime, creation_date_val)

    # å®‰å…¨åœ°æå–å…¶ä»–å­—æ®µ
    source_name = str(row.get("source_name", "Unknown"))
    source_version = str(row.get("source_version", "1.0"))
    device = str(row.get("device", "Unknown"))

    unit_val = row.get("unit")
    unit = (
      str(unit_val)
      if unit_val is not None
      and str(unit_val).lower() not in ("", "nan", "none")
      else None
    )

    value_val = row.get("value")
    value = (
      float(value_val)
      if value_val is not None
      and str(value_val).lower() not in ("", "nan", "none")
      else None
    )

    metadata = row.get("metadata", {})
    if metadata is None:
      metadata = {}

    return cls(
      type=record_type,
      source_name=source_name,
      start_date=start_date,
      end_date=end_date,
      creation_date=creation_date,
      source_version=source_version,
      device=device,
      unit=unit,
      value=value,
      metadata=metadata,
    )

  def to_health_record(self) -> HealthRecord:
    """è½¬æ¢ä¸º HealthRecord"""
    # å¦‚æœ end_date ä¸º Noneï¼Œä½¿ç”¨ start_date ä½œä¸ºé»˜è®¤å€¼
    end_date = self.end_date if self.end_date is not None else self.start_date

    if self.value is not None:
      # åˆ›å»º QuantityRecord
      from src.core.data_models import QuantityRecord

      return QuantityRecord(
        type=self.type,
        source_name=self.source_name,
        start_date=self.start_date,
        end_date=end_date,  # ç°åœ¨ä¿è¯ä¸ä¸º None
        creation_date=self.creation_date,
        source_version=self.source_version,
        device=self.device,
        unit=self.unit,
        value=self.value,
        metadata=self.metadata,
      )
    else:
      # åˆ›å»ºåŸºç¡€ HealthRecord
      return HealthRecord(
        type=self.type,
        source_name=self.source_name,
        start_date=self.start_date,
        end_date=end_date,  # ç°åœ¨ä¿è¯ä¸ä¸º None
        creation_date=self.creation_date,
        source_version=self.source_version,
        device=self.device,
        unit=self.unit,
        metadata=self.metadata,
      )


class DataQualityReport(BaseModel):
  """æ•°æ®è´¨é‡æŠ¥å‘Š"""

  total_records: int
  valid_records: int
  invalid_records: int
  duplicate_records: int
  cleaned_records: int
  quality_score: float  # 0-100

  # è¯¦ç»†ç»Ÿè®¡
  timestamp_issues: int = 0
  value_issues: int = 0
  metadata_issues: int = 0

  # æ•°æ®åˆ†å¸ƒ
  source_distribution: dict[str, int] = Field(default_factory=dict)
  type_distribution: dict[str, int] = Field(default_factory=dict)

  # æ—¶é—´èŒƒå›´
  date_range: dict[str, datetime | None] = Field(
    default_factory=lambda: {"start": None, "end": None}
  )


class DeduplicationResult(BaseModel):
  """å»é‡ç»“æœ"""

  original_count: int
  deduplicated_count: int
  removed_duplicates: int
  strategy_used: str
  processing_time_seconds: float

  # è¯¦ç»†ç»Ÿè®¡
  duplicates_by_source: dict[str, int] = Field(default_factory=dict)
  time_windows_processed: int = 0


class DataCleaner:
  """æ•°æ®æ¸…æ´—æ ¸å¿ƒç±»

  æä¾›å¤šç§æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†åŠŸèƒ½ï¼š
  - æ—¶é—´çª—å£å»é‡
  - æ•°æ®æºä¼˜å…ˆçº§å¤„ç†
  - å åŠ æ•°æ®åˆå¹¶
  - æ•°æ®è´¨é‡éªŒè¯
  """

  def __init__(
    self,
    source_priority: dict[str, int] | None = None,
    default_window_seconds: int = 60,
  ):
    """
    åˆå§‹åŒ–æ•°æ®æ¸…æ´—å™¨

    Args:
        source_priority: æ•°æ®æºä¼˜å…ˆçº§æ˜ å°„ï¼Œè¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
            ä¾‹å¦‚: {"ğŸ™Watch": 1, "å°ç±³è¿åŠ¨å¥åº·": 2, "ğŸ™Phone": 3}
        default_window_seconds: é»˜è®¤æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
    """
    # é»˜è®¤æ•°æ®æºä¼˜å…ˆçº§ï¼ˆæ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼‰
    self.source_priority = source_priority or {
      "ğŸ™Watch": 1,  # Apple Watch æœ€é«˜ä¼˜å…ˆçº§
      "Apple Watch": 1,  # åˆ«å
      "å°ç±³è¿åŠ¨å¥åº·": 2,  # å°ç±³è¿åŠ¨å¥åº·
      "Xiaomi Home": 2,  # åˆ«å
      "ğŸ™Phone": 3,  # iPhone æœ€ä½ä¼˜å…ˆçº§
      "iPhone": 3,  # åˆ«å
    }

    self.default_window_seconds = default_window_seconds
    logger.info(
      f"DataCleaner initialized with {len(self.source_priority)} source priorities"
    )

  def deduplicate_by_time_window(
    self,
    records: list[HealthRecord],
    window_seconds: int | None = None,
    strategy: str = "priority",
  ) -> tuple[list[HealthRecord], DeduplicationResult]:
    """
    åŸºäºæ—¶é—´çª—å£çš„å»é‡å¤„ç†

    Args:
        records: å¾…å¤„ç†çš„è®°å½•åˆ—è¡¨
        window_seconds: æ—¶é—´çª—å£å¤§å°ï¼ˆç§’ï¼‰ï¼ŒNone ä½¿ç”¨é»˜è®¤å€¼
        strategy: å»é‡ç­–ç•¥
            - "priority": æŒ‰æ•°æ®æºä¼˜å…ˆçº§ä¿ç•™
            - "latest": ä¿ç•™æœ€æ–°çš„è®°å½•
            - "average": è®¡ç®—å¹³å‡å€¼ï¼ˆä»…æ•°å€¼ç±»å‹ï¼‰
            - "highest_quality": åŸºäºè´¨é‡è¯„åˆ†ä¿ç•™

    Returns:
        (å»é‡åçš„è®°å½•åˆ—è¡¨, å»é‡ç»“æœç»Ÿè®¡)
    """
    if not records:
      return [], DeduplicationResult(
        original_count=0,
        deduplicated_count=0,
        removed_duplicates=0,
        strategy_used=strategy,
        processing_time_seconds=0.0,
      )

    start_time = datetime.now()
    window = window_seconds or self.default_window_seconds

    logger.info(
      f"Starting deduplication with strategy '{strategy}', window {window}s"
    )

    # æŒ‰è®°å½•ç±»å‹åˆ†ç»„å¤„ç†
    records_by_type = defaultdict(list)
    for record in records:
      records_by_type[record.type].append(record)

    deduplicated_records = []
    total_duplicates_removed = 0
    duplicates_by_source = defaultdict(int)

    for record_type, type_records in records_by_type.items():
      logger.debug(
        f"Processing {len(type_records)} records of type {record_type}"
      )

      # è½¬æ¢ä¸º DataFrame ä¾¿äºå¤„ç†
      df = self._records_to_dataframe(type_records)

      # ç¡®ä¿ start_date æ˜¯ datetime ç±»å‹
      df["start_date"] = pd.to_datetime(df["start_date"])

      # æŒ‰æ—¶é—´çª—å£åˆ†ç»„
      df["time_window"] = df["start_date"].dt.floor(f"{window}s").astype(str)

      # å¯¹æ¯ä¸ªæ—¶é—´çª—å£å†…çš„è®°å½•è¿›è¡Œå»é‡
      cleaned_groups = []
      groupby_result = list(df.groupby("time_window"))
      windows_processed = len(groupby_result)  # ç¡®ä¿å˜é‡æ€»æ˜¯è¢«èµ‹å€¼

      for _window_start, group in groupby_result:
        if len(group) == 1:
          # åªæœ‰ä¸€ä¸ªè®°å½•ï¼Œæ— éœ€å»é‡
          cleaned_groups.append(group.iloc[0])
          continue

        # å¤šä¸ªè®°å½•ï¼Œéœ€è¦å»é‡
        cleaned_record, duplicates_removed = self._deduplicate_group(
          group, strategy, record_type
        )

        if cleaned_record is not None:
          cleaned_groups.append(cleaned_record)

        total_duplicates_removed += duplicates_removed
        # ç»Ÿè®¡è¢«ç§»é™¤çš„è®°å½•æŒ‰æ•°æ®æºåˆ†å¸ƒ
        if cleaned_record is not None:
          cleaned_id = cleaned_record.name  # DataFrame index
          for _, row in group.iterrows():
            if row.name != cleaned_id:
              duplicates_by_source[row["source_name"]] += 1

      # è½¬æ¢å›è®°å½•å¯¹è±¡
      for cleaned_row in cleaned_groups:
        record = self._dataframe_row_to_record(cleaned_row, record_type)
        if record:
          deduplicated_records.append(record)

    processing_time = (datetime.now() - start_time).total_seconds()

    result = DeduplicationResult(
      original_count=len(records),
      deduplicated_count=len(deduplicated_records),
      removed_duplicates=total_duplicates_removed,
      strategy_used=strategy,
      processing_time_seconds=processing_time,
      duplicates_by_source=dict(duplicates_by_source),
      time_windows_processed=windows_processed,
    )

    logger.info(
      f"Deduplication completed: {result.original_count} -> "
      f"{result.deduplicated_count} records "
      f"({result.removed_duplicates} duplicates removed)"
    )

    return deduplicated_records, result

  def merge_overlapping_records(
    self, records: list[HealthRecord], merge_threshold_seconds: int = 5
  ) -> list[HealthRecord]:
    """
    åˆå¹¶é‡å æˆ–ç›¸é‚»çš„è®°å½•

    ä¸»è¦ç”¨äºç¡çœ æ•°æ®å’Œè¿åŠ¨æ•°æ®ï¼Œè¿™äº›æ•°æ®å¯èƒ½è¢«åˆ†å‰²æˆå¤šä¸ªè¿ç»­çš„è®°å½•ã€‚

    Args:
        records: å¾…åˆå¹¶çš„è®°å½•åˆ—è¡¨
        merge_threshold_seconds: åˆå¹¶é˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œè®°å½•é—´éš”å°äºæ­¤å€¼åˆ™åˆå¹¶

    Returns:
        åˆå¹¶åçš„è®°å½•åˆ—è¡¨
    """
    if not records or len(records) <= 1:
      return records

    logger.info(
      f"Merging overlapping records, threshold: {merge_threshold_seconds}s"
    )

    # æŒ‰è®°å½•ç±»å‹åˆ†ç»„
    records_by_type = defaultdict(list)
    for record in records:
      records_by_type[record.type].append(record)

    merged_records = []

    for record_type, type_records in records_by_type.items():
      if not self._should_merge_type(record_type):
        # è¯¥ç±»å‹ä¸éœ€è¦åˆå¹¶
        merged_records.extend(type_records)
        continue

      # æ’åºå¹¶åˆå¹¶
      sorted_records = sorted(type_records, key=lambda r: r.start_date)
      merged = self._merge_sorted_records(
        sorted_records, merge_threshold_seconds
      )
      merged_records.extend(merged)

    logger.info(
      f"Merge completed: {len(records)} -> {len(merged_records)} records"
    )
    return merged_records

  def validate_data_quality(
    self, records: list[HealthRecord]
  ) -> DataQualityReport:
    """
    éªŒè¯æ•°æ®è´¨é‡å¹¶ç”ŸæˆæŠ¥å‘Š

    Args:
        records: å¾…éªŒè¯çš„è®°å½•åˆ—è¡¨

    Returns:
        æ•°æ®è´¨é‡æŠ¥å‘Š
    """
    if not records:
      return DataQualityReport(
        total_records=0,
        valid_records=0,
        invalid_records=0,
        duplicate_records=0,
        cleaned_records=0,
        quality_score=0.0,
      )

    logger.info(f"Validating data quality for {len(records)} records")

    total_records = len(records)
    valid_records = 0
    invalid_records = 0

    # è¯¦ç»†ç»Ÿè®¡
    timestamp_issues = 0
    value_issues = 0
    metadata_issues = 0

    # åˆ†å¸ƒç»Ÿè®¡
    source_distribution = defaultdict(int)
    type_distribution = defaultdict(int)

    # æ—¶é—´èŒƒå›´
    dates = []

    for record in records:
      is_valid = True

      # æ£€æŸ¥æ—¶é—´æˆ³
      if not self._validate_timestamp(record):
        timestamp_issues += 1
        is_valid = False

      # æ£€æŸ¥æ•°å€¼
      if not self._validate_value(record):
        value_issues += 1
        is_valid = False

      # æ£€æŸ¥å…ƒæ•°æ®
      if not self._validate_metadata(record):
        metadata_issues += 1
        # å…ƒæ•°æ®é—®é¢˜ä¸å½±å“è®°å½•æœ‰æ•ˆæ€§ï¼Œåªè®°å½•ç»Ÿè®¡

      if is_valid:
        valid_records += 1
      else:
        invalid_records += 1

      # ç»Ÿè®¡åˆ†å¸ƒ
      source_distribution[record.source_name] += 1
      type_distribution[record.type] += 1

      # æ”¶é›†æ—¥æœŸ
      dates.append(record.start_date)

    # è®¡ç®—è´¨é‡è¯„åˆ†
    quality_score = self._calculate_quality_score(
      total_records, valid_records, timestamp_issues, value_issues
    )

    # æ—¶é—´èŒƒå›´
    date_range = {
      "start": min(dates) if dates else None,
      "end": max(dates) if dates else None,
    }

    # æ£€æµ‹é‡å¤ï¼ˆç®€å•æ£€æµ‹ï¼ŒåŸºäºæ—¶é—´å’Œå€¼å®Œå…¨ç›¸åŒï¼‰
    duplicate_records = self._detect_duplicates(records)

    report = DataQualityReport(
      total_records=total_records,
      valid_records=valid_records,
      invalid_records=invalid_records,
      duplicate_records=duplicate_records,
      cleaned_records=valid_records,  # å‡è®¾æ¸…ç†åä¿ç•™æœ‰æ•ˆè®°å½•
      quality_score=quality_score,
      timestamp_issues=timestamp_issues,
      value_issues=value_issues,
      metadata_issues=metadata_issues,
      source_distribution=dict(source_distribution),
      type_distribution=dict(type_distribution),
      date_range=date_range,
    )

    logger.info(
      f"Quality validation completed: {valid_records}/{total_records} valid "
      f"(score: {quality_score:.1f})"
    )

    return report

  def _records_to_dataframe(self, records: list[HealthRecord]) -> pd.DataFrame:
    """å°†è®°å½•åˆ—è¡¨è½¬æ¢ä¸º DataFrame"""
    data = []
    for record in records:
      row = {
        "id": id(record),  # ä½¿ç”¨å¯¹è±¡IDä½œä¸ºå”¯ä¸€æ ‡è¯†
        "type": record.type,
        "source_name": record.source_name,
        "start_date": record.start_date,
        "end_date": record.end_date,
        "creation_date": record.creation_date,
        "value": getattr(record, "value", None),
        "unit": getattr(record, "unit", None),
        "metadata": getattr(record, "metadata", None),
      }
      data.append(row)

    return pd.DataFrame(data)

  def _deduplicate_group(
    self, group: pd.DataFrame, strategy: str, record_type: str
  ) -> tuple[pd.Series | None, int]:
    """
    å¯¹å•ä¸ªæ—¶é—´çª—å£å†…çš„è®°å½•ç»„è¿›è¡Œå»é‡

    Returns:
        (ä¿ç•™çš„è®°å½•è¡Œ, ç§»é™¤çš„é‡å¤è®°å½•æ•°)
    """
    if len(group) <= 1:
      return group.iloc[0] if len(group) == 1 else None, 0

    if strategy == "priority":
      return self._deduplicate_by_priority(group), len(group) - 1
    elif strategy == "latest":
      return self._deduplicate_by_latest(group), len(group) - 1
    elif strategy == "average" and self._is_numeric_type(record_type):
      return self._deduplicate_by_average(group), len(group) - 1
    elif strategy == "highest_quality":
      return self._deduplicate_by_quality(group), len(group) - 1
    else:
      # é»˜è®¤ä½¿ç”¨ä¼˜å…ˆçº§ç­–ç•¥
      return self._deduplicate_by_priority(group), len(group) - 1

  def _deduplicate_by_priority(self, group: pd.DataFrame) -> pd.Series:
    """æŒ‰æ•°æ®æºä¼˜å…ˆçº§å»é‡"""

    # ä¸ºæ¯æ¡è®°å½•è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°
    def get_priority_score(source_name: str) -> int:
      return self.source_priority.get(source_name, 999)  # é»˜è®¤æœ€ä½ä¼˜å…ˆçº§

    group = group.copy()
    group["priority_score"] = group["source_name"].apply(get_priority_score)

    # é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜çš„è®°å½•ï¼ˆåˆ†æ•°æœ€å°ï¼‰
    idx = group["priority_score"].idxmin()
    if idx is not None:
      result = group.loc[idx]
      assert isinstance(result, pd.Series)  # ç±»å‹ç»†åŒ–
      return result
    else:
      result = group.iloc[0]
      assert isinstance(result, pd.Series)  # ç±»å‹ç»†åŒ–
      return result

  def _deduplicate_by_latest(self, group: pd.DataFrame) -> pd.Series:
    """ä¿ç•™æœ€æ–°çš„è®°å½•"""
    idx = group["creation_date"].idxmax()
    if idx is not None:
      result = group.loc[idx]
      assert isinstance(result, pd.Series)  # ç±»å‹ç»†åŒ–
      return result
    else:
      result = group.iloc[0]
      assert isinstance(result, pd.Series)  # ç±»å‹ç»†åŒ–
      return result

  def _deduplicate_by_average(self, group: pd.DataFrame) -> pd.Series:
    """è®¡ç®—å¹³å‡å€¼ï¼ˆä»…æ•°å€¼ç±»å‹ï¼‰"""
    result = group.iloc[0].copy()  # ä½¿ç”¨ç¬¬ä¸€æ¡è®°å½•ä½œä¸ºæ¨¡æ¿

    # è®¡ç®—æ•°å€¼å¹³å‡å€¼
    numeric_values = group["value"].dropna()
    if not numeric_values.empty:
      result["value"] = numeric_values.mean()

    # æ›´æ–°å…ƒæ•°æ®è¡¨ç¤ºè¿™æ˜¯å¹³å‡å€¼
    result["metadata"] = {
      **(result["metadata"] or {}),
      "deduplication_method": "average",
      "original_records_count": len(group),
      "averaged_values_str": str(
        list(numeric_values)
      ),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²é¿å…åºåˆ—åŒ–é—®é¢˜
    }

    return result

  def _deduplicate_by_quality(self, group: pd.DataFrame) -> pd.Series:
    """åŸºäºè´¨é‡è¯„åˆ†å»é‡"""

    def calculate_quality_score(row) -> float:
      score = 0.0

      # å…ƒæ•°æ®å®Œæ•´æ€§ (0-30åˆ†)
      metadata = row.get("metadata", {}) or {}
      completeness = len(metadata) / 10  # å‡è®¾æœ€å¤š10ä¸ªå…ƒæ•°æ®å­—æ®µ
      score += min(completeness * 30, 30)

      # æ•°æ®æºä¼˜å…ˆçº§ (0-40åˆ†)
      priority = self.source_priority.get(row["source_name"], 999)
      priority_score = max(
        0, 40 - priority * 10
      )  # ä¼˜å…ˆçº§1å¾—40åˆ†ï¼Œä¼˜å…ˆçº§2å¾—30åˆ†ç­‰
      score += priority_score

      # æ—¶é—´æˆ³åˆç†æ€§ (0-30åˆ†)
      if pd.notna(row.get("creation_date")) and pd.notna(row.get("start_date")):
        time_diff = abs(
          (row["creation_date"] - row["start_date"]).total_seconds()
        )
        if time_diff < 86400:  # 24å°æ—¶å†…
          score += 30
        elif time_diff < 604800:  # 7å¤©å†…
          score += 20

      return score

    group = group.copy()
    group["quality_score"] = group.apply(calculate_quality_score, axis=1)

    # é€‰æ‹©è´¨é‡è¯„åˆ†æœ€é«˜çš„è®°å½•
    idx = group["quality_score"].idxmax()
    if idx is not None:
      result = group.loc[idx]
      assert isinstance(result, pd.Series)  # ç±»å‹ç»†åŒ–
      return result
    else:
      result = group.iloc[0]
      assert isinstance(result, pd.Series)  # ç±»å‹ç»†åŒ–
      return result

  def _dataframe_row_to_record(
    self, row: pd.Series, record_type: str
  ) -> HealthRecord | None:
    """å°† DataFrame è¡Œè½¬æ¢å›è®°å½•å¯¹è±¡"""
    try:
      # ä½¿ç”¨ RecordRowData ä¸­é—´ç±»è¿›è¡Œç±»å‹å®‰å…¨çš„è½¬æ¢
      row_data = RecordRowData.from_series(row, record_type)
      return row_data.to_health_record()

    except Exception as e:
      logger.error(f"Failed to reconstruct record: {e}")
      return None

  def _should_merge_type(self, record_type: str) -> bool:
    """åˆ¤æ–­è®°å½•ç±»å‹æ˜¯å¦éœ€è¦åˆå¹¶"""
    # ç¡çœ è®°å½•å’ŒæŸäº›è¿åŠ¨è®°å½•éœ€è¦åˆå¹¶
    merge_types = {
      "HKCategoryTypeIdentifierSleepAnalysis",
      "HKWorkoutTypeIdentifier",  # è¿åŠ¨è®°å½•
    }
    return record_type in merge_types

  def _merge_sorted_records(
    self, records: list[HealthRecord], threshold_seconds: int
  ) -> list[HealthRecord]:
    """åˆå¹¶å·²æ’åºçš„è®°å½•åˆ—è¡¨"""
    if not records:
      return []

    merged = [records[0]]

    for current in records[1:]:
      last = merged[-1]

      # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶
      if self._can_merge_records(last, current, threshold_seconds):
        # åˆå¹¶è®°å½•
        merged[-1] = self._merge_two_records(last, current)
      else:
        # ä¸èƒ½åˆå¹¶ï¼Œæ·»åŠ ä¸ºæ–°è®°å½•
        merged.append(current)

    return merged

  def _can_merge_records(
    self, record1: HealthRecord, record2: HealthRecord, threshold_seconds: int
  ) -> bool:
    """åˆ¤æ–­ä¸¤æ¡è®°å½•æ˜¯å¦å¯ä»¥åˆå¹¶"""
    # æ—¶é—´ä¸Šè¿ç»­æˆ–è½»å¾®é‡å 
    time_gap = (record2.start_date - record1.end_date).total_seconds()
    return time_gap <= threshold_seconds

  def _merge_two_records(
    self, record1: HealthRecord, record2: HealthRecord
  ) -> HealthRecord:
    """åˆå¹¶ä¸¤æ¡è®°å½•"""
    # åˆ›å»ºåˆå¹¶åçš„è®°å½•
    # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“è®°å½•ç±»å‹å®ç°åˆå¹¶é€»è¾‘
    # æš‚æ—¶è¿”å›ç¬¬ä¸€æ¡è®°å½•
    logger.warning("Record merging not fully implemented")
    return record1

  def _validate_timestamp(self, record: HealthRecord) -> bool:
    """éªŒè¯æ—¶é—´æˆ³æœ‰æ•ˆæ€§"""
    try:
      # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦å­˜åœ¨
      if not hasattr(record, "start_date") or not record.start_date:
        return False

      # æ£€æŸ¥æ—¶é—´æˆ³åˆç†æ€§ï¼ˆä¸èƒ½æ˜¯æœªæ¥å¤ªè¿œçš„æ—¥æœŸï¼‰
      now = datetime.now(record.start_date.tzinfo)
      if record.start_date > now + timedelta(days=1):
        return False

      # æ£€æŸ¥å¼€å§‹æ—¶é—´ä¸èƒ½æ™šäºç»“æŸæ—¶é—´
      if hasattr(record, "end_date") and record.end_date:
        if record.start_date > record.end_date:
          return False

      return True
    except Exception:
      return False

  def _validate_value(self, record: HealthRecord) -> bool:
    """éªŒè¯æ•°å€¼æœ‰æ•ˆæ€§"""
    try:
      # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹è®°å½•
      if not self._is_numeric_type(record.type):
        return True  # éæ•°å€¼ç±»å‹è®°å½•ä¸éœ€è¦æ•°å€¼éªŒè¯

      # æ£€æŸ¥æ˜¯å¦æœ‰ value å±æ€§
      if not hasattr(record, "value"):
        return False  # æ•°å€¼ç±»å‹è®°å½•å¿…é¡»æœ‰ value

      value = getattr(record, "value", None)
      if value is None:
        return False  # æ•°å€¼ç±»å‹è®°å½•çš„ value ä¸èƒ½ä¸º None

      # åŸºæœ¬æ•°å€¼æ£€æŸ¥
      if not isinstance(value, (int, float)):
        return False

      # é’ˆå¯¹ä¸åŒè®°å½•ç±»å‹çš„ç‰¹æ®Šæ£€æŸ¥
      if record.type == "HKQuantityTypeIdentifierHeartRate":
        # å¿ƒç‡åº”è¯¥åœ¨ 30-250 bpm ä¹‹é—´
        return 30 <= value <= 250
      elif record.type == "HKQuantityTypeIdentifierBodyMass":
        # ä½“é‡åº”è¯¥åœ¨ 20-300 kg ä¹‹é—´
        return 20 <= value <= 300

      # å…¶ä»–ç±»å‹ä½¿ç”¨é€šç”¨æ£€æŸ¥
      return abs(value) < 1e10  # é¿å…æç«¯å€¼

    except Exception:
      return False

  def _validate_metadata(self, record: HealthRecord) -> bool:
    """éªŒè¯å…ƒæ•°æ®æœ‰æ•ˆæ€§"""
    try:
      if not hasattr(record, "metadata"):
        return True

      metadata = record.metadata
      if metadata is None:
        return True

      # æ£€æŸ¥å…ƒæ•°æ®æ˜¯å¦ä¸ºå­—å…¸
      if not isinstance(metadata, dict):
        return False

      # æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬çš„å…ƒæ•°æ®å­—æ®µ
      # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„å…ƒæ•°æ®éªŒè¯é€»è¾‘

      return True
    except Exception:
      return False

  def _calculate_quality_score(
    self, total: int, valid: int, timestamp_issues: int, value_issues: int
  ) -> float:
    """è®¡ç®—è´¨é‡è¯„åˆ† (0-100)"""
    if total == 0:
      return 0.0

    # æœ‰æ•ˆæ€§è¯„åˆ† (60% æƒé‡)
    validity_score = (valid / total) * 60

    # é—®é¢˜ä¸¥é‡ç¨‹åº¦è¯„åˆ† (40% æƒé‡)
    issue_penalty = ((timestamp_issues + value_issues) / total) * 40

    return max(0.0, min(100.0, validity_score - issue_penalty))

  def _detect_duplicates(self, records: list[HealthRecord]) -> int:
    """ç®€å•é‡å¤æ£€æµ‹"""
    seen = set()
    duplicates = 0

    for record in records:
      # åˆ›å»ºè®°å½•çš„ç­¾åï¼ˆç±»å‹ + æ—¶é—´ + å€¼ï¼‰
      signature = (
        record.type,
        record.start_date.isoformat(),
        getattr(record, "value", None),
        record.source_name,
      )

      if signature in seen:
        duplicates += 1
      else:
        seen.add(signature)

    return duplicates

  def _is_numeric_type(self, record_type: str) -> bool:
    """åˆ¤æ–­è®°å½•ç±»å‹æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹"""
    numeric_types = {
      "HKQuantityTypeIdentifierHeartRate",
      "HKQuantityTypeIdentifierBodyMass",
      "HKQuantityTypeIdentifierBodyMassIndex",
      "HKQuantityTypeIdentifierHeight",
      "HKQuantityTypeIdentifierHeartRateVariabilitySDNN",
    }
    return record_type in numeric_types
