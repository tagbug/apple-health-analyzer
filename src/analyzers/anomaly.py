"""å¼‚å¸¸æ£€æµ‹æ¨¡å— - æä¾›å¤šç§å¼‚å¸¸æ£€æµ‹ç®—æ³•"""

from collections.abc import Sequence
from dataclasses import dataclass
from datetime import datetime
from typing import Literal, TypedDict

import numpy as np
import pandas as pd

from ..core.data_models import CategoryRecord, HealthRecord, QuantityRecord
from ..utils.logger import get_logger

logger = get_logger(__name__)


class SeverityThresholds(TypedDict):
  """ä¸¥é‡ç¨‹åº¦é˜ˆå€¼é…ç½®"""

  low: float
  medium: float
  high: float


class AnomalyConfig(TypedDict, total=False):
  """å¼‚å¸¸æ£€æµ‹é…ç½®"""

  zscore_threshold: float
  iqr_multiplier: float
  ma_threshold: float
  context_threshold: float
  severity_thresholds: SeverityThresholds


@dataclass
class AnomalyRecord:
  """å¼‚å¸¸è®°å½•æ•°æ®ç±»"""

  timestamp: datetime
  value: float
  expected_value: float  # é¢„æœŸå€¼
  deviation: float  # åå·®ç¨‹åº¦
  severity: Literal["low", "medium", "high"]  # ä¸¥é‡ç¨‹åº¦
  method: str  # æ£€æµ‹æ–¹æ³•
  confidence: float  # ç½®ä¿¡åº¦ (0-1)
  context: dict[str, str | float | int]  # ä¸Šä¸‹æ–‡ä¿¡æ¯


@dataclass
class AnomalyReport:
  """å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š"""

  total_records: int
  anomaly_count: int
  anomaly_rate: float
  anomalies_by_severity: dict[str, int]
  anomalies_by_method: dict[str, int]
  time_distribution: dict[str, dict[str, int]]  # å¼‚å¸¸çš„æ—¶é—´åˆ†å¸ƒ
  recommendations: list[str]  # æ”¹è¿›å»ºè®®


class AnomalyDetector:
  """å¼‚å¸¸æ£€æµ‹æ ¸å¿ƒç±»"""

  def __init__(self, config: AnomalyConfig | None = None):
    """åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨

    Args:
        config: æ£€æµ‹é…ç½®å‚æ•°
    """
    default_config: AnomalyConfig = {
      "zscore_threshold": 3.0,  # Z-Score é˜ˆå€¼
      "iqr_multiplier": 1.5,  # IQR å€æ•°
      "ma_threshold": 2.0,  # ç§»åŠ¨å¹³å‡é˜ˆå€¼
      "context_threshold": 2.5,  # ä¸Šä¸‹æ–‡å¼‚å¸¸é˜ˆå€¼
      "severity_thresholds": {  # ä¸¥é‡ç¨‹åº¦é˜ˆå€¼
        "low": 1.5,
        "medium": 2.5,
        "high": 3.5,
      },
    }

    self.config = default_config
    if config:
      self.config.update(config)

    logger.info("AnomalyDetector initialized")

  def detect_anomalies(
    self,
    records: Sequence[HealthRecord],
    methods: list[Literal["zscore", "iqr", "moving_average", "contextual"]]
    | None = None,
    context: Literal[
      "time_of_day", "day_of_week", "sleep_wake"
    ] = "time_of_day",
  ) -> list[AnomalyRecord]:
    """æ£€æµ‹å¼‚å¸¸å€¼

    Args:
        records: å¥åº·è®°å½•åˆ—è¡¨
        methods: æ£€æµ‹æ–¹æ³•åˆ—è¡¨
        context: ä¸Šä¸‹æ–‡ç±»å‹ (ç”¨äºä¸Šä¸‹æ–‡å¼‚å¸¸æ£€æµ‹)

    Returns:
        å¼‚å¸¸è®°å½•åˆ—è¡¨
    """
    if not records:
      logger.warning("No records provided for anomaly detection")
      return []

    if methods is None:
      methods = ["zscore", "iqr"]

    logger.info(
      f"Detecting anomalies in {len(records)} records using methods: {methods}"
    )

    # è½¬æ¢ä¸ºDataFrame
    df = self._records_to_dataframe(records)

    if df.empty or "value" not in df.columns:
      logger.warning("No valid data for anomaly detection")
      return []

    all_anomalies = []

    # ä½¿ç”¨ä¸åŒæ–¹æ³•æ£€æµ‹å¼‚å¸¸
    for method in methods:
      try:
        if method == "zscore":
          anomalies = self._detect_zscore(df)
        elif method == "iqr":
          anomalies = self._detect_iqr(df)
        elif method == "moving_average":
          anomalies = self._detect_moving_average(df)
        elif method == "contextual":
          anomalies = self._detect_contextual(df, context)
        else:
          logger.warning(f"Unknown detection method: {method}")
          continue

        all_anomalies.extend(anomalies)
        logger.debug(f"Method {method} found {len(anomalies)} anomalies")

      except Exception as e:
        logger.error(f"Error in {method} detection: {e}")
        continue

    # å»é‡ (åŒä¸€ä¸ªæ—¶é—´ç‚¹çš„å¼‚å¸¸åªä¿ç•™æœ€ä¸¥é‡çš„)
    unique_anomalies = self._deduplicate_anomalies(all_anomalies)

    logger.info(f"Total unique anomalies detected: {len(unique_anomalies)}")
    return unique_anomalies

  def generate_report(
    self, anomalies: Sequence[AnomalyRecord], total_records: int
  ) -> AnomalyReport:
    """ç”Ÿæˆå¼‚å¸¸æ£€æµ‹æŠ¥å‘Š

    Args:
        anomalies: å¼‚å¸¸è®°å½•åˆ—è¡¨
        total_records: æ€»è®°å½•æ•°

    Returns:
        å¼‚å¸¸æ£€æµ‹æŠ¥å‘Š
    """
    anomaly_count = len(anomalies)
    anomaly_rate = anomaly_count / total_records if total_records > 0 else 0

    # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
    by_severity = {
      "low": sum(1 for a in anomalies if a.severity == "low"),
      "medium": sum(1 for a in anomalies if a.severity == "medium"),
      "high": sum(1 for a in anomalies if a.severity == "high"),
    }

    # æŒ‰æ–¹æ³•åˆ†ç±»
    by_method = {}
    for anomaly in anomalies:
      by_method[anomaly.method] = by_method.get(anomaly.method, 0) + 1

    # æ—¶é—´åˆ†å¸ƒåˆ†æ
    time_distribution = self._analyze_time_distribution(anomalies)

    # ç”Ÿæˆå»ºè®®
    recommendations = self._generate_recommendations(anomalies, anomaly_rate)

    return AnomalyReport(
      total_records=total_records,
      anomaly_count=anomaly_count,
      anomaly_rate=round(anomaly_rate, 4),
      anomalies_by_severity=by_severity,
      anomalies_by_method=by_method,
      time_distribution=time_distribution,
      recommendations=recommendations,
    )

  def _detect_zscore(self, df: pd.DataFrame) -> list[AnomalyRecord]:
    """Z-Score å¼‚å¸¸æ£€æµ‹

    åŸç†: (x - Î¼) / Ïƒ > threshold
    é€‚ç”¨: æ•°æ®è¿‘ä¼¼æ­£æ€åˆ†å¸ƒæ—¶æ•ˆæœæœ€å¥½
    """
    values = df["value"].dropna()
    if len(values) < 3:
      return []

    mean_val = values.mean()
    std_val = values.std()

    if std_val == 0:
      return []

    threshold = self.config["zscore_threshold"]
    anomalies = []

    for _idx, row in df.iterrows():
      if pd.isna(row["value"]):
        continue

      z_score = abs(row["value"] - mean_val) / std_val

      if z_score > threshold:
        severity = self._calculate_severity(z_score)
        confidence = min(1.0, z_score / 5.0)  # åŸºäºZ-Scoreè®¡ç®—ç½®ä¿¡åº¦

        anomalies.append(
          AnomalyRecord(
            timestamp=row["start_date"],
            value=row["value"],
            expected_value=mean_val,
            deviation=z_score,
            severity=severity,
            method="zscore",
            confidence=round(confidence, 3),
            context={
              "mean": round(mean_val, 2),
              "std": round(std_val, 2),
              "z_score": round(z_score, 2),
            },
          )
        )

    return anomalies

  def _detect_iqr(self, df: pd.DataFrame) -> list[AnomalyRecord]:
    """IQR å››åˆ†ä½è·å¼‚å¸¸æ£€æµ‹

    åŸç†: Q1 - k*IQR < x < Q3 + k*IQR
    ä¼˜åŠ¿: å¯¹æç«¯å€¼ä¸æ•æ„Ÿï¼Œæ›´é²æ£’
    """
    values = df["value"].dropna()
    if len(values) < 4:  # éœ€è¦è‡³å°‘4ä¸ªå€¼è®¡ç®—å››åˆ†ä½æ•°
      return []

    Q1 = values.quantile(0.25)
    Q3 = values.quantile(0.75)
    IQR = Q3 - Q1

    if IQR == 0:
      return []

    multiplier = self.config["iqr_multiplier"]
    lower_bound = Q1 - multiplier * IQR
    upper_bound = Q3 + multiplier * IQR

    anomalies = []

    for _idx, row in df.iterrows():
      if pd.isna(row["value"]):
        continue

      value = row["value"]

      if value < lower_bound or value > upper_bound:
        # è®¡ç®—åå·®ç¨‹åº¦ (ç›¸å¯¹äºIQRçš„å€æ•°)
        if value < lower_bound:
          deviation = (lower_bound - value) / IQR if IQR > 0 else 0
        else:
          deviation = (value - upper_bound) / IQR if IQR > 0 else 0

        severity = self._calculate_severity(deviation)
        confidence = min(1.0, deviation / 3.0)  # åŸºäºIQRå€æ•°è®¡ç®—ç½®ä¿¡åº¦

        anomalies.append(
          AnomalyRecord(
            timestamp=row["start_date"],
            value=row["value"],
            expected_value=float(np.asarray((Q1 + Q3) / 2)),  # ä¸­ä½æ•°ä½œä¸ºé¢„æœŸå€¼
            deviation=deviation,
            severity=severity,
            method="iqr",
            confidence=round(confidence, 3),
            context={
              "Q1": round(float(np.asarray(Q1)), 2),
              "Q3": round(float(np.asarray(Q3)), 2),
              "IQR": round(float(np.asarray(IQR)), 2),
              "lower_bound": round(float(np.asarray(lower_bound)), 2),
              "upper_bound": round(float(np.asarray(upper_bound)), 2),
            },
          )
        )

    return anomalies

  def _detect_moving_average(
    self, df: pd.DataFrame, window: int = 7
  ) -> list[AnomalyRecord]:
    """ç§»åŠ¨å¹³å‡å¼‚å¸¸æ£€æµ‹

    åŸç†: å½“å‰å€¼ä¸ç§»åŠ¨å¹³å‡å€¼åå·® > threshold * std
    ä¼˜åŠ¿: æ•æ‰çŸ­æœŸå¼‚å¸¸æ³¢åŠ¨
    """
    if len(df) < window:
      return []

    # è®¡ç®—ç§»åŠ¨å¹³å‡å’Œç§»åŠ¨æ ‡å‡†å·®
    df = df.copy().sort_values("start_date")
    df["ma"] = df["value"].rolling(window=window, center=True).mean()
    df["ma_std"] = df["value"].rolling(window=window, center=True).std()

    threshold = self.config["ma_threshold"]
    anomalies = []

    for _idx, row in df.iterrows():
      if pd.isna(row["ma"]) or pd.isna(row["ma_std"]) or row["ma_std"] == 0:
        continue

      deviation = abs(row["value"] - row["ma"])
      threshold_value = threshold * row["ma_std"]

      if deviation > threshold_value:
        severity = self._calculate_severity(deviation / row["ma_std"])
        confidence = min(1.0, deviation / (3 * row["ma_std"]))

        anomalies.append(
          AnomalyRecord(
            timestamp=row["start_date"],
            value=row["value"],
            expected_value=float(np.asarray(row["ma"])),
            deviation=deviation / float(np.asarray(row["ma_std"])),
            severity=severity,
            method="moving_average",
            confidence=round(confidence, 3),
            context={
              "moving_average": round(float(np.asarray(row["ma"])), 2),
              "ma_std": round(float(np.asarray(row["ma_std"])), 2),
              "window": window,
            },
          )
        )

    return anomalies

  def _detect_contextual(
    self,
    df: pd.DataFrame,
    context: Literal["time_of_day", "day_of_week", "sleep_wake"],
  ) -> list[AnomalyRecord]:
    """ä¸Šä¸‹æ–‡å¼‚å¸¸æ£€æµ‹

    åŸºäºæ—¶é—´æ¨¡å¼çš„å¼‚å¸¸æ£€æµ‹
    """
    if context == "time_of_day":
      return self._detect_time_of_day_anomalies(df)
    elif context == "day_of_week":
      return self._detect_day_of_week_anomalies(df)
    elif context == "sleep_wake":
      return self._detect_sleep_wake_anomalies(df)
    else:
      logger.warning(f"Unknown context type: {context}")
      return []

  def _detect_time_of_day_anomalies(
    self, df: pd.DataFrame
  ) -> list[AnomalyRecord]:
    """æŒ‰å°æ—¶çš„å¼‚å¸¸æ£€æµ‹"""
    df = df.copy()
    df["hour"] = df["start_date"].dt.hour

    # è®¡ç®—æ¯ä¸ªå°æ—¶çš„ç»Ÿè®¡å€¼
    hourly_stats = df.groupby("hour")["value"].agg(["mean", "std"]).dropna()

    threshold = self.config["context_threshold"]
    anomalies = []

    for _idx, row in df.iterrows():
      hour = row["hour"]

      if hour not in hourly_stats.index:
        continue

      mean_val = hourly_stats.loc[hour, "mean"]
      std_val = hourly_stats.loc[hour, "std"]

      if std_val == 0:
        continue

      z_score = abs(row["value"] - mean_val) / std_val

      if z_score > threshold:
        severity = self._calculate_severity(z_score)
        confidence = min(1.0, z_score / 4.0)

        # å®‰å…¨åœ°å¤„ç†pandas Scalarç±»å‹
        mean_val_float = float(np.asarray(mean_val))
        std_val_float = float(np.asarray(std_val))

        anomalies.append(
          AnomalyRecord(
            timestamp=row["start_date"],
            value=row["value"],
            expected_value=mean_val_float,
            deviation=z_score,
            severity=severity,
            method="contextual_time_of_day",
            confidence=round(confidence, 3),
            context={
              "hour": hour,
              "hourly_mean": round(mean_val_float, 2),
              "hourly_std": round(std_val_float, 2),
            },
          )
        )

    return anomalies

  def _detect_day_of_week_anomalies(
    self, df: pd.DataFrame
  ) -> list[AnomalyRecord]:
    """æŒ‰æ˜ŸæœŸçš„å¼‚å¸¸æ£€æµ‹"""
    df = df.copy()
    df["day_of_week"] = df["start_date"].dt.dayofweek  # 0=Monday, 6=Sunday

    # è®¡ç®—æ¯å‘¨æ¯ä¸€å¤©çš„ç»Ÿè®¡å€¼
    daily_stats = (
      df.groupby("day_of_week")["value"].agg(["mean", "std"]).dropna()
    )

    threshold = self.config["context_threshold"]
    anomalies = []

    for _idx, row in df.iterrows():
      day = row["day_of_week"]

      if day not in daily_stats.index:
        continue

      mean_val = daily_stats.loc[day, "mean"]
      std_val = daily_stats.loc[day, "std"]

      if std_val == 0:
        continue

      z_score = abs(row["value"] - mean_val) / std_val

      if z_score > threshold:
        severity = self._calculate_severity(z_score)
        confidence = min(1.0, z_score / 4.0)

        day_names = [
          "Monday",
          "Tuesday",
          "Wednesday",
          "Thursday",
          "Friday",
          "Saturday",
          "Sunday",
        ]

        anomalies.append(
          AnomalyRecord(
            timestamp=row["start_date"],
            value=row["value"],
            expected_value=float(np.asarray(mean_val)),
            deviation=z_score,
            severity=severity,
            method="contextual_day_of_week",
            confidence=round(confidence, 3),
            context={
              "day_of_week": day,
              "day_name": day_names[day],
              "daily_mean": round(float(np.asarray(mean_val)), 2),
              "daily_std": round(float(np.asarray(std_val)), 2),
            },
          )
        )

    return anomalies

  def _detect_sleep_wake_anomalies(
    self, df: pd.DataFrame
  ) -> list[AnomalyRecord]:
    """ç¡çœ /æ¸…é†’çŠ¶æ€å¼‚å¸¸æ£€æµ‹"""
    # è¿™éœ€è¦ç¡çœ æ•°æ®ï¼Œç›®å‰ç®€åŒ–å®ç°
    # å®é™…å®ç°éœ€è¦ç»“åˆç¡çœ è®°å½•
    logger.info("Sleep/wake anomaly detection not yet implemented")
    return []

  def _calculate_severity(
    self, deviation: float
  ) -> Literal["low", "medium", "high"]:
    """æ ¹æ®åå·®ç¨‹åº¦è®¡ç®—ä¸¥é‡æ€§"""
    thresholds = self.config["severity_thresholds"]

    if deviation >= thresholds["high"]:
      return "high"
    elif deviation >= thresholds["medium"]:
      return "medium"
    else:
      return "low"

  def _deduplicate_anomalies(
    self, anomalies: Sequence[AnomalyRecord]
  ) -> list[AnomalyRecord]:
    """å»é‡å¼‚å¸¸è®°å½•ï¼Œä¿ç•™æœ€ä¸¥é‡çš„"""
    if not anomalies:
      return []

    # æŒ‰æ—¶é—´æˆ³åˆ†ç»„
    by_timestamp = {}
    for anomaly in anomalies:
      timestamp = anomaly.timestamp
      if timestamp not in by_timestamp:
        by_timestamp[timestamp] = []
      by_timestamp[timestamp].append(anomaly)

    # å¯¹æ¯ä¸ªæ—¶é—´æˆ³ä¿ç•™æœ€ä¸¥é‡çš„å¼‚å¸¸
    unique_anomalies = []
    severity_order = {"low": 1, "medium": 2, "high": 3}

    for _timestamp, anomaly_list in by_timestamp.items():
      # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åºï¼Œå–æœ€ä¸¥é‡çš„
      most_severe = max(anomaly_list, key=lambda x: severity_order[x.severity])
      unique_anomalies.append(most_severe)

    return unique_anomalies

  def _analyze_time_distribution(
    self, anomalies: Sequence[AnomalyRecord]
  ) -> dict[str, dict[str, int]]:
    """åˆ†æå¼‚å¸¸çš„æ—¶é—´åˆ†å¸ƒ"""
    if not anomalies:
      return {}

    distribution: dict[str, dict[str, int]] = {
      "by_hour": {},
      "by_day_of_week": {},
      "by_month": {},
    }

    for anomaly in anomalies:
      # æŒ‰å°æ—¶åˆ†å¸ƒ
      hour = anomaly.timestamp.hour
      distribution["by_hour"][str(hour)] = (
        distribution["by_hour"].get(str(hour), 0) + 1
      )

      # æŒ‰æ˜ŸæœŸåˆ†å¸ƒ
      day_of_week = anomaly.timestamp.weekday()
      day_names = [
        "Monday",
        "Tuesday",
        "Wednesday",
        "Thursday",
        "Friday",
        "Saturday",
        "Sunday",
      ]
      day_name = day_names[day_of_week]
      distribution["by_day_of_week"][day_name] = (
        distribution["by_day_of_week"].get(day_name, 0) + 1
      )

      # æŒ‰æœˆä»½åˆ†å¸ƒ
      month = anomaly.timestamp.month
      distribution["by_month"][str(month)] = (
        distribution["by_month"].get(str(month), 0) + 1
      )

    return distribution

  def _generate_recommendations(
    self, anomalies: Sequence[AnomalyRecord], anomaly_rate: float
  ) -> list[str]:
    """ç”Ÿæˆå¼‚å¸¸æ£€æµ‹å»ºè®®"""
    recommendations = []

    if anomaly_rate > 0.1:  # å¼‚å¸¸ç‡è¶…è¿‡10%
      recommendations.append("âš ï¸ å¼‚å¸¸ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è°ƒæ•´æ£€æµ‹é˜ˆå€¼")

    if anomaly_rate < 0.001:  # å¼‚å¸¸ç‡è¿‡ä½
      recommendations.append("â„¹ï¸ æ£€æµ‹åˆ°çš„å¼‚å¸¸è¾ƒå°‘ï¼Œå¯èƒ½é˜ˆå€¼è®¾ç½®è¿‡é«˜")

    # åˆ†æä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
    high_severity = sum(1 for a in anomalies if a.severity == "high")
    if high_severity > len(anomalies) * 0.3:
      recommendations.append("ğŸš¨ é«˜ä¸¥é‡ç¨‹åº¦å¼‚å¸¸è¾ƒå¤šï¼Œå»ºè®®é‡ç‚¹å…³æ³¨")

    # åˆ†ææ—¶é—´åˆ†å¸ƒ
    if anomalies:
      time_dist = self._analyze_time_distribution(anomalies)

      # æ£€æŸ¥æ˜¯å¦é›†ä¸­åœ¨ç‰¹å®šæ—¶é—´
      hour_counts = time_dist.get("by_hour", {})
      max_hour_count = max(hour_counts.values()) if hour_counts else 0
      if max_hour_count > len(anomalies) * 0.5:
        recommendations.append("ğŸ“Š å¼‚å¸¸ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šå°æ—¶ï¼Œå¯èƒ½æ˜¯æ­£å¸¸æ¨¡å¼")

    return recommendations

  def _records_to_dataframe(
    self, records: Sequence[HealthRecord]
  ) -> pd.DataFrame:
    """å°†å¥åº·è®°å½•è½¬æ¢ä¸ºDataFrame"""
    data = []
    for record in records:
      # è·å–æ•°å€¼ (åªå¤„ç†æœ‰æ•°å€¼çš„è®°å½•)
      value = None
      # æ£€æŸ¥æ˜¯å¦æ˜¯QuantityRecordæˆ–CategoryRecordå­ç±»ï¼Œè¿™äº›ç±»æœ‰valueå±æ€§
      if isinstance(record, (QuantityRecord, CategoryRecord)):
        value = record.value

      data.append(
        {
          "type": record.type,
          "source_name": record.source_name,
          "start_date": record.start_date,
          "end_date": record.end_date,
          "value": value,
          "unit": record.unit,
        }
      )

    return pd.DataFrame(data)
