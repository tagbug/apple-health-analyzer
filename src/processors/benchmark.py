"""Performance benchmark module.

Measures Apple Health analysis performance and generates CLI reports.
"""

import os
import threading
import time
from collections.abc import Callable
from pathlib import Path
from typing import Any

import psutil
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from ..analyzers.highlights import HighlightsGenerator
from ..analyzers.statistical import StatisticalAnalyzer
from ..core.xml_parser import StreamingXMLParser
from ..utils.logger import get_logger
from .cleaner import DataCleaner
from .exporter import DataExporter

logger = get_logger(__name__)


class TimeoutError(Exception):
  """Timeout exception for benchmark runs."""


class BenchmarkModule:
  """Single benchmark module definition."""

  def __init__(self, name: str, test_func: Callable, description: str = ""):
    self.name = name
    self.test_func = test_func
    self.description = description or name


class BenchmarkResult:
  """Benchmark result payload."""

  def __init__(self, module_name: str):
    self.module_name = module_name
    self.status = "pending"  # pending, completed, timeout, error
    self.time_seconds = 0.0
    self.memory_mb = 0.0
    self.records_processed = 0
    self.throughput_records_per_sec = 0.0
    self.error_message = ""


class BenchmarkRunner:
  """Benchmark runner for performance tests."""

  def __init__(
    self,
    xml_path: Path,
    output_dir: Path | None = None,
    timeout_seconds: int = 30,
  ):
    self.xml_path = xml_path
    self.output_dir = output_dir if output_dir is not None else Path("output")
    self.output_dir.mkdir(exist_ok=True)
    self.timeout_seconds = timeout_seconds
    self.sample_records: list[Any] = []
    self.results: list[BenchmarkResult] = []
    self.start_time = time.time()

  def get_memory_usage(self) -> float:
    """Return current process memory usage in MB."""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

  def _run_with_timeout(self, func: Callable, *args, **kwargs) -> Any:
    """Run a function and stop it if it exceeds the timeout."""
    result = [None]
    exception: list[Exception | None] = [None]

    def target():
      try:
        result[0] = func(*args, **kwargs)
      except Exception as e:
        exception[0] = e

    thread = threading.Thread(target=target)
    thread.daemon = True
    thread.start()
    thread.join(self.timeout_seconds)

    if thread.is_alive():
      logger.warning(f"æµ‹è¯•è¶…æ—¶ ({self.timeout_seconds}s)ï¼Œå¼ºåˆ¶åœæ­¢")
      raise TimeoutError(f"æµ‹è¯•è¶…æ—¶ ({self.timeout_seconds}s)")

    if exception[0]:
      raise exception[0]

    return result[0]

  def load_sample_data(self, limit: int = 10000) -> tuple[list[Any], dict[str, Any]]:
    """Load a sample of records from XML and return parse metrics."""
    logger.info(f"ä»XMLæ–‡ä»¶å¼€å¤´åŠ è½½å‰{limit}æ¡æ•°æ®ä½œä¸ºæµ‹è¯•æ ·æœ¬...")

    sample_records = []
    parser = StreamingXMLParser(self.xml_path)
    count = 0

    # Record XML parsing metrics.
    start_time = time.time()
    start_mem = self.get_memory_usage()

    for record in parser.parse_records():
      sample_records.append(record)
      count += 1
      if count >= limit:
        break

    end_time = time.time()
    end_mem = self.get_memory_usage()

    # Compute parsing throughput metrics.
    parse_time = end_time - start_time
    parse_metrics = {
      "records_processed": len(sample_records),
      "throughput_records_per_sec": len(sample_records) / parse_time
      if parse_time > 0
      else 0,
      "memory_delta_mb": end_mem - start_mem,
      "parse_time_seconds": parse_time,
    }

    logger.info(f"å·²åŠ è½½ {len(sample_records)} æ¡çœŸå®æ•°æ®ç”¨äºæµ‹è¯•")
    logger.info(f"XMLè§£ææ€§èƒ½: {parse_metrics['throughput_records_per_sec']:.0f} æ¡/ç§’")

    self.sample_records = sample_records
    return sample_records, parse_metrics

  def run_module_with_timeout(
    self, module: BenchmarkModule, *args, **kwargs
  ) -> BenchmarkResult:
    """Run a module with timeout handling."""
    result = BenchmarkResult(module.name)

    try:
      logger.info(f"å¼€å§‹æµ‹è¯•æ¨¡å—: {module.name}")

      start_time = time.time()
      start_mem = self.get_memory_usage()

      # Run the test function.
      test_result = self._run_with_timeout(module.test_func, *args, **kwargs)

      end_time = time.time()
      end_mem = self.get_memory_usage()

      # Record successful results.
      result.status = "completed"
      result.time_seconds = end_time - start_time
      result.memory_mb = end_mem - start_mem

      # Extract metrics from the test result.
      if isinstance(test_result, dict):
        result.records_processed = test_result.get("records_processed", 0)
        result.throughput_records_per_sec = test_result.get(
          "throughput_records_per_sec", 0.0
        )

      logger.info(f"æµ‹è¯•æ¨¡å— {module.name} å®Œæˆ: {result.time_seconds:.2f}s")

    except TimeoutError:
      result.status = "timeout"
      result.time_seconds = self.timeout_seconds
      logger.warning(f"æµ‹è¯•æ¨¡å— {module.name} è¶…æ—¶")

    except Exception as e:
      result.status = "error"
      result.error_message = str(e)
      logger.error(f"æµ‹è¯•æ¨¡å— {module.name} å‡ºé”™: {e}")

    return result

  def benchmark_data_cleaning(self, sample_records: list[Any]) -> dict[str, Any]:
    """Benchmark data cleaning performance."""
    start_time = time.time()
    start_mem = self.get_memory_usage()

    cleaner = DataCleaner()
    cleaned_records, dedup_result = cleaner.deduplicate_by_time_window(
      sample_records, window_seconds=300
    )

    end_time = time.time()
    end_mem = self.get_memory_usage()

    # Ensure a minimum elapsed time to avoid division by zero.
    elapsed_time = max(end_time - start_time, 0.001)  # Minimum 1 ms.

    return {
      "records_processed": len(sample_records),
      "throughput_records_per_sec": len(sample_records) / elapsed_time,
      "memory_delta_mb": end_mem - start_mem,
      "cleaned_records": len(cleaned_records),
      "duplicates_removed": dedup_result.removed_duplicates,
    }

  def benchmark_statistical_analysis(self, sample_records: list[Any]) -> dict[str, Any]:
    """Benchmark statistical analysis performance."""
    start_time = time.time()
    start_mem = self.get_memory_usage()

    analyzer = StatisticalAnalyzer()
    analyzer.generate_report(sample_records)

    end_time = time.time()
    end_mem = self.get_memory_usage()

    # Ensure a minimum elapsed time to avoid division by zero.
    elapsed_time = max(end_time - start_time, 0.001)  # Minimum 1 ms.

    return {
      "records_processed": len(sample_records),
      "throughput_records_per_sec": len(sample_records) / elapsed_time,
      "memory_delta_mb": end_mem - start_mem,
    }

  def benchmark_report_generation(self, sample_records: list[Any]) -> dict[str, Any]:
    """Benchmark report generation performance."""
    start_time = time.time()
    start_mem = self.get_memory_usage()

    # Simulate report generation.
    highlights_gen = HighlightsGenerator()
    _highlights = highlights_gen.generate_comprehensive_highlights()

    end_time = time.time()
    end_mem = self.get_memory_usage()

    # Ensure a minimum elapsed time to avoid division by zero.
    elapsed_time = max(end_time - start_time, 0.001)  # Minimum 1 ms.

    return {
      "records_processed": len(sample_records),
      "throughput_records_per_sec": len(sample_records) / elapsed_time,
      "memory_delta_mb": end_mem - start_mem,
    }

  def benchmark_data_export(self, sample_records: list[Any]) -> dict[str, Any]:
    """Benchmark data export performance."""
    start_time = time.time()
    start_mem = self.get_memory_usage()

    output_path = self.output_dir / "benchmark_export_sample.csv"
    exporter = DataExporter(self.output_dir)
    exporter.export_to_csv(sample_records, output_path)

    end_time = time.time()
    end_mem = self.get_memory_usage()

    # Ensure a minimum elapsed time to avoid division by zero.
    elapsed_time = max(end_time - start_time, 0.001)  # Minimum 1 ms.

    return {
      "records_processed": len(sample_records),
      "throughput_records_per_sec": len(sample_records) / elapsed_time,
      "memory_delta_mb": end_mem - start_mem,
      "output_file": str(output_path),
      "file_size_mb": output_path.stat().st_size / 1024 / 1024,
    }

  def run_all_benchmarks(self) -> list[BenchmarkResult]:
    """Run all benchmark modules."""
    logger.info("=== å¼€å§‹å®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯• ===")

    # 1. Load sample data and gather XML parse metrics.
    sample_records, xml_parse_metrics = self.load_sample_data(10000)
    if not sample_records:
      logger.error("æ— æ³•åŠ è½½æ ·æœ¬æ•°æ®ï¼Œæµ‹è¯•ç»ˆæ­¢")
      return []

    # 2. Define benchmark modules.
    modules = [
      BenchmarkModule(
        "æ•°æ®æ¸…æ´—", self.benchmark_data_cleaning, "æµ‹è¯•æ•°æ®å»é‡å’Œæ¸…æ´—æ€§èƒ½"
      ),
      BenchmarkModule(
        "ç»Ÿè®¡åˆ†æ", self.benchmark_statistical_analysis, "æµ‹è¯•ç»Ÿè®¡åˆ†æè®¡ç®—æ€§èƒ½"
      ),
      BenchmarkModule(
        "æŠ¥å‘Šç”Ÿæˆ", self.benchmark_report_generation, "æµ‹è¯•å¥åº·æŠ¥å‘Šç”Ÿæˆæ€§èƒ½"
      ),
      BenchmarkModule("æ•°æ®å¯¼å‡º", self.benchmark_data_export, "æµ‹è¯•æ•°æ®å¯¼å‡ºåˆ°æ–‡ä»¶æ€§èƒ½"),
    ]

    # 3. Run all benchmark modules.
    results = []

    # Add XML parse results first (from load_sample_data).
    xml_result = BenchmarkResult("XML è§£æ")
    xml_result.status = "completed"
    xml_result.time_seconds = xml_parse_metrics["parse_time_seconds"]
    xml_result.memory_mb = xml_parse_metrics["memory_delta_mb"]
    xml_result.records_processed = xml_parse_metrics["records_processed"]
    xml_result.throughput_records_per_sec = xml_parse_metrics[
      "throughput_records_per_sec"
    ]
    results.append(xml_result)

    # Run the remaining modules.
    for module in modules:
      result = self.run_module_with_timeout(module, sample_records)
      results.append(result)

    self.results = results

    # 4. Compute overall statistics.
    completed_count = sum(1 for r in results if r.status == "completed")
    total_time = time.time() - self.start_time

    logger.info("=== æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ ===")
    logger.info(f"æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f} ç§’")
    logger.info(f"æ ·æœ¬æ•°æ®é‡: {len(sample_records)} æ¡è®°å½•")
    logger.info(f"å®Œæˆæ¨¡å—: {completed_count}/{len(results)}")

    return results

  def print_report(self):
    """Print the benchmark report."""
    if not self.results:
      logger.error("æ²¡æœ‰å¯ç”¨çš„åŸºå‡†æµ‹è¯•ç»“æœï¼Œè¯·å…ˆè¿è¡Œ run_all_benchmarks()")
      return

    console = Console()
    total_time = time.time() - self.start_time
    completed_count = sum(1 for r in self.results if r.status == "completed")

    # Title panel.
    title = Text("ğŸ Apple Health Analyzer - æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š", style="bold blue")
    console.print(Panel(title, border_style="blue", padding=(1, 2)))

    # Summary table.
    info_table = Table(show_header=True, header_style="bold cyan", box=None)
    info_table.add_column("æŒ‡æ ‡", style="dim", width=10)
    info_table.add_column("æ•°å€¼", style="green")

    info_table.add_row("æµ‹è¯•å¼€å§‹æ—¶é—´", time.strftime("%Y-%m-%d %H:%M:%S"))
    info_table.add_row("æ€»æµ‹è¯•æ—¶é—´", f"{total_time:.2f} ç§’")
    info_table.add_row("æ ·æœ¬æ•°æ®é‡", f"{len(self.sample_records):,} æ¡è®°å½•")
    info_table.add_row("è¶…æ—¶è®¾ç½®", f"{self.timeout_seconds} ç§’")
    info_table.add_row("å®Œæˆæ¨¡å—", f"{completed_count}/{len(self.results)}")

    console.print(info_table)
    console.print()

    # Performance table.
    perf_table = Table(
      title="ğŸ” å„æ¨¡å—æ€§èƒ½æŒ‡æ ‡",
      title_style="bold yellow",
      show_header=True,
      header_style="bold magenta",
      border_style="blue",
    )

    perf_table.add_column("æ¨¡å—åç§°", style="cyan", min_width=12)
    perf_table.add_column("çŠ¶æ€", style="green", min_width=6, justify="center")
    perf_table.add_column("æ—¶é—´(ç§’)", style="yellow", min_width=10, justify="right")
    perf_table.add_column("è®°å½•æ•°", style="blue", min_width=10, justify="right")
    perf_table.add_column("ååé‡(æ¡/ç§’)", style="red", min_width=15, justify="right")
    perf_table.add_column("å†…å­˜å¢é‡(MB)", style="purple", min_width=12, justify="right")

    for result in self.results:
      # Status icons and colors.
      status_config = {
        "completed": ("âœ…", "green"),
        "timeout": ("â°", "yellow"),
        "error": ("âŒ", "red"),
        "pending": ("â³", "blue"),
      }
      status_icon, status_color = status_config.get(result.status, ("â“", "white"))

      # Format throughput to avoid extreme values.
      throughput = result.throughput_records_per_sec
      if throughput > 1000000:  # å¦‚æœååé‡è¶…è¿‡100ä¸‡ï¼Œæ˜¾ç¤ºä¸º"ç¬æ—¶"
        throughput_str = Text("ç¬æ—¶", style="bold green")
      else:
        throughput_str = f"{throughput:,.0f}"

      # å†…å­˜å¢é‡é¢œè‰²ï¼ˆæ­£æ•°çº¢è‰²è¡¨ç¤ºå¢åŠ ï¼Œè´Ÿæ•°ç»¿è‰²è¡¨ç¤ºå‡å°‘ï¼‰
      memory_color = "red" if result.memory_mb > 0 else "green"
      memory_str = f"{result.memory_mb:+.2f}"

      perf_table.add_row(
        result.module_name,
        Text(status_icon, style=status_color),
        f"{result.time_seconds:.2f}",
        f"{result.records_processed:,}",
        throughput_str,
        Text(memory_str, style=memory_color),
      )

    console.print(perf_table)

    # Bottleneck analysis.
    if completed_count > 0:
      console.print("\nğŸ’¡ [bold cyan]æ€§èƒ½ç“¶é¢ˆåˆ†æ:[/bold cyan]")

      # Find the slowest module.
      sorted_by_time = sorted(
        [r for r in self.results if r.status == "completed"],
        key=lambda x: x.time_seconds,
        reverse=True,
      )
      if sorted_by_time:
        slowest = sorted_by_time[0]
        console.print(
          f"  âš ï¸  [red]{slowest.module_name}[/red]æ¨¡å—è€—æ—¶æœ€é•¿ï¼ˆ[bold]{slowest.time_seconds:.2f}ç§’[/bold]ï¼‰"
        )

      # Find the lowest throughput module.
      sorted_by_throughput = sorted(
        [r for r in self.results if r.status == "completed"],
        key=lambda x: x.throughput_records_per_sec,
      )
      if (
        sorted_by_throughput and sorted_by_throughput[0].throughput_records_per_sec > 0
      ):
        lowest_throughput = sorted_by_throughput[0]
        console.print(
          f"  âš ï¸  [red]{lowest_throughput.module_name}[/red]æ¨¡å—ååé‡æœ€ä½ï¼ˆ[bold]{lowest_throughput.throughput_records_per_sec:,.0f}æ¡/ç§’[/bold]ï¼‰"
        )

      # Find the highest memory usage module.
      sorted_by_memory = sorted(
        [r for r in self.results if r.status == "completed"],
        key=lambda x: x.memory_mb,
        reverse=True,
      )
      if sorted_by_memory and sorted_by_memory[0].memory_mb > 10:
        highest_memory = sorted_by_memory[0]
        memory_color = "red" if highest_memory.memory_mb > 0 else "green"
        console.print(
          f"  âš ï¸  [red]{highest_memory.module_name}[/red]æ¨¡å—å†…å­˜å ç”¨æœ€é«˜ï¼ˆ[bold {memory_color}]{highest_memory.memory_mb:.1f} MB[/bold {memory_color}]ï¼‰"
        )

    # Completion timestamp.
    console.print(
      f"\nâœ… [green]æµ‹è¯•å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}[/green]"
    )


def run_benchmark(xml_path: str, output_dir: str | None = None, timeout: int = 30):
  """Convenience runner for performance benchmarks."""
  xml_path_obj = Path(xml_path)
  if not xml_path_obj.exists():
    raise FileNotFoundError(f"XMLæ–‡ä»¶ä¸å­˜åœ¨: {xml_path}")

  output_dir_obj = Path(output_dir) if output_dir else None

  runner = BenchmarkRunner(xml_path_obj, output_dir_obj, timeout)
  results = runner.run_all_benchmarks()
  runner.print_report()

  return results
