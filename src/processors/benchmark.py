"""
æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡å—
æµ‹è¯•Apple Healthæ•°æ®åˆ†æå™¨çš„å„é¡¹æ€§èƒ½æŒ‡æ ‡
é€šè¿‡CLIå‘½ä»¤è°ƒç”¨ï¼Œç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
"""

import os
import threading
import time
from collections.abc import Callable
from pathlib import Path
from typing import Any

import psutil
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from ..analyzers.highlights import HighlightsGenerator
from ..analyzers.statistical import StatisticalAnalyzer
from ..core.xml_parser import StreamingXMLParser
from ..utils.logger import get_logger
from .cleaner import DataCleaner
from .exporter import DataExporter

logger = get_logger(__name__)


class TimeoutError(Exception):
  """è¶…æ—¶å¼‚å¸¸"""

  pass


class BenchmarkModule:
  """å•ä¸ªåŸºå‡†æµ‹è¯•æ¨¡å—"""

  def __init__(self, name: str, test_func: Callable, description: str = ""):
    self.name = name
    self.test_func = test_func
    self.description = description or name


class BenchmarkResult:
  """åŸºå‡†æµ‹è¯•ç»“æœ"""

  def __init__(self, module_name: str):
    self.module_name = module_name
    self.status = "pending"  # pending, completed, timeout, error
    self.time_seconds = 0.0
    self.memory_mb = 0.0
    self.records_processed = 0
    self.throughput_records_per_sec = 0.0
    self.error_message = ""


class BenchmarkRunner:
  """æ€§èƒ½åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""

  def __init__(
    self,
    xml_path: Path,
    output_dir: Path | None = None,
    timeout_seconds: int = 30,
  ):
    self.xml_path = xml_path
    self.output_dir = output_dir if output_dir is not None else Path("output")
    self.output_dir.mkdir(exist_ok=True)
    self.timeout_seconds = timeout_seconds
    self.sample_records: list[Any] = []
    self.results: list[BenchmarkResult] = []
    self.start_time = time.time()

  def get_memory_usage(self) -> float:
    """è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

  def _run_with_timeout(self, func: Callable, *args, **kwargs) -> Any:
    """è¿è¡Œå‡½æ•°å¹¶åœ¨è¶…æ—¶åå¼ºåˆ¶åœæ­¢"""
    result = [None]
    exception: list[Exception | None] = [None]

    def target():
      try:
        result[0] = func(*args, **kwargs)
      except Exception as e:
        exception[0] = e

    thread = threading.Thread(target=target)
    thread.daemon = True
    thread.start()
    thread.join(self.timeout_seconds)

    if thread.is_alive():
      logger.warning(f"æµ‹è¯•è¶…æ—¶ ({self.timeout_seconds}s)ï¼Œå¼ºåˆ¶åœæ­¢")
      raise TimeoutError(f"æµ‹è¯•è¶…æ—¶ ({self.timeout_seconds}s)")

    if exception[0]:
      raise exception[0]

    return result[0]

  def load_sample_data(
    self, limit: int = 10000
  ) -> tuple[list[Any], dict[str, Any]]:
    """ä»XMLæ–‡ä»¶å¼€å¤´åŠ è½½æŒ‡å®šæ•°é‡çš„æ ·æœ¬æ•°æ®ï¼Œå¹¶è¿”å›è§£ææ€§èƒ½æŒ‡æ ‡"""
    logger.info(f"ä»XMLæ–‡ä»¶å¼€å¤´åŠ è½½å‰{limit}æ¡æ•°æ®ä½œä¸ºæµ‹è¯•æ ·æœ¬...")

    sample_records = []
    parser = StreamingXMLParser(self.xml_path)
    count = 0

    # è®°å½•XMLè§£ææ€§èƒ½
    start_time = time.time()
    start_mem = self.get_memory_usage()

    for record in parser.parse_records():
      sample_records.append(record)
      count += 1
      if count >= limit:
        break

    end_time = time.time()
    end_mem = self.get_memory_usage()

    # è®¡ç®—è§£ææ€§èƒ½æŒ‡æ ‡
    parse_time = end_time - start_time
    parse_metrics = {
      "records_processed": len(sample_records),
      "throughput_records_per_sec": len(sample_records) / parse_time
      if parse_time > 0
      else 0,
      "memory_delta_mb": end_mem - start_mem,
      "parse_time_seconds": parse_time,
    }

    logger.info(f"å·²åŠ è½½ {len(sample_records)} æ¡çœŸå®æ•°æ®ç”¨äºæµ‹è¯•")
    logger.info(
      f"XMLè§£ææ€§èƒ½: {parse_metrics['throughput_records_per_sec']:.0f} æ¡/ç§’"
    )

    self.sample_records = sample_records
    return sample_records, parse_metrics

  def run_module_with_timeout(
    self, module: BenchmarkModule, *args, **kwargs
  ) -> BenchmarkResult:
    """è¿è¡Œå•ä¸ªæµ‹è¯•æ¨¡å—å¹¶å¤„ç†è¶…æ—¶"""
    result = BenchmarkResult(module.name)

    try:
      logger.info(f"å¼€å§‹æµ‹è¯•æ¨¡å—: {module.name}")

      start_time = time.time()
      start_mem = self.get_memory_usage()

      # è¿è¡Œæµ‹è¯•å‡½æ•°
      test_result = self._run_with_timeout(module.test_func, *args, **kwargs)

      end_time = time.time()
      end_mem = self.get_memory_usage()

      # è®°å½•æˆåŠŸç»“æœ
      result.status = "completed"
      result.time_seconds = end_time - start_time
      result.memory_mb = end_mem - start_mem

      # ä»æµ‹è¯•ç»“æœä¸­æå–æŒ‡æ ‡
      if isinstance(test_result, dict):
        result.records_processed = test_result.get("records_processed", 0)
        result.throughput_records_per_sec = test_result.get(
          "throughput_records_per_sec", 0.0
        )

      logger.info(f"æµ‹è¯•æ¨¡å— {module.name} å®Œæˆ: {result.time_seconds:.2f}s")

    except TimeoutError:
      result.status = "timeout"
      result.time_seconds = self.timeout_seconds
      logger.warning(f"æµ‹è¯•æ¨¡å— {module.name} è¶…æ—¶")

    except Exception as e:
      result.status = "error"
      result.error_message = str(e)
      logger.error(f"æµ‹è¯•æ¨¡å— {module.name} å‡ºé”™: {e}")

    return result

  def benchmark_data_cleaning(
    self, sample_records: list[Any]
  ) -> dict[str, Any]:
    """æµ‹è¯•æ•°æ®æ¸…æ´—æ€§èƒ½"""
    start_time = time.time()
    start_mem = self.get_memory_usage()

    cleaner = DataCleaner()
    cleaned_records, dedup_result = cleaner.deduplicate_by_time_window(
      sample_records, window_seconds=300
    )

    end_time = time.time()
    end_mem = self.get_memory_usage()

    # ç¡®ä¿æœ€å°æ—¶é—´é—´éš”ï¼Œé¿å…é™¤é›¶é”™è¯¯
    elapsed_time = max(end_time - start_time, 0.001)  # æœ€å°1ms

    return {
      "records_processed": len(sample_records),
      "throughput_records_per_sec": len(sample_records) / elapsed_time,
      "memory_delta_mb": end_mem - start_mem,
      "cleaned_records": len(cleaned_records),
      "duplicates_removed": dedup_result.removed_duplicates,
    }

  def benchmark_statistical_analysis(
    self, sample_records: list[Any]
  ) -> dict[str, Any]:
    """æµ‹è¯•ç»Ÿè®¡åˆ†ææ€§èƒ½"""
    start_time = time.time()
    start_mem = self.get_memory_usage()

    analyzer = StatisticalAnalyzer()
    analyzer.generate_report(sample_records)

    end_time = time.time()
    end_mem = self.get_memory_usage()

    # ç¡®ä¿æœ€å°æ—¶é—´é—´éš”ï¼Œé¿å…é™¤é›¶é”™è¯¯
    elapsed_time = max(end_time - start_time, 0.001)  # æœ€å°1ms

    return {
      "records_processed": len(sample_records),
      "throughput_records_per_sec": len(sample_records) / elapsed_time,
      "memory_delta_mb": end_mem - start_mem,
    }

  def benchmark_report_generation(
    self, sample_records: list[Any]
  ) -> dict[str, Any]:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆæ€§èƒ½"""
    start_time = time.time()
    start_mem = self.get_memory_usage()

    # æ¨¡æ‹ŸæŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹
    highlights_gen = HighlightsGenerator()
    _highlights = highlights_gen.generate_comprehensive_highlights()

    end_time = time.time()
    end_mem = self.get_memory_usage()

    # ç¡®ä¿æœ€å°æ—¶é—´é—´éš”ï¼Œé¿å…é™¤é›¶é”™è¯¯
    elapsed_time = max(end_time - start_time, 0.001)  # æœ€å°1ms

    return {
      "records_processed": len(sample_records),
      "throughput_records_per_sec": len(sample_records) / elapsed_time,
      "memory_delta_mb": end_mem - start_mem,
    }

  def benchmark_data_export(self, sample_records: list[Any]) -> dict[str, Any]:
    """æµ‹è¯•æ•°æ®å¯¼å‡ºæ€§èƒ½"""
    start_time = time.time()
    start_mem = self.get_memory_usage()

    output_path = self.output_dir / "benchmark_export_sample.csv"
    exporter = DataExporter(self.output_dir)
    exporter.export_to_csv(sample_records, output_path)

    end_time = time.time()
    end_mem = self.get_memory_usage()

    # ç¡®ä¿æœ€å°æ—¶é—´é—´éš”ï¼Œé¿å…é™¤é›¶é”™è¯¯
    elapsed_time = max(end_time - start_time, 0.001)  # æœ€å°1ms

    return {
      "records_processed": len(sample_records),
      "throughput_records_per_sec": len(sample_records) / elapsed_time,
      "memory_delta_mb": end_mem - start_mem,
      "output_file": str(output_path),
      "file_size_mb": output_path.stat().st_size / 1024 / 1024,
    }

  def run_all_benchmarks(self) -> list[BenchmarkResult]:
    """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
    logger.info("=== å¼€å§‹å®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯• ===")

    # 1. åŠ è½½æ ·æœ¬æ•°æ®å¹¶è·å–XMLè§£ææ€§èƒ½æŒ‡æ ‡
    sample_records, xml_parse_metrics = self.load_sample_data(10000)
    if not sample_records:
      logger.error("æ— æ³•åŠ è½½æ ·æœ¬æ•°æ®ï¼Œæµ‹è¯•ç»ˆæ­¢")
      return []

    # 2. å®šä¹‰æµ‹è¯•æ¨¡å—
    modules = [
      BenchmarkModule(
        "æ•°æ®æ¸…æ´—", self.benchmark_data_cleaning, "æµ‹è¯•æ•°æ®å»é‡å’Œæ¸…æ´—æ€§èƒ½"
      ),
      BenchmarkModule(
        "ç»Ÿè®¡åˆ†æ", self.benchmark_statistical_analysis, "æµ‹è¯•ç»Ÿè®¡åˆ†æè®¡ç®—æ€§èƒ½"
      ),
      BenchmarkModule(
        "æŠ¥å‘Šç”Ÿæˆ", self.benchmark_report_generation, "æµ‹è¯•å¥åº·æŠ¥å‘Šç”Ÿæˆæ€§èƒ½"
      ),
      BenchmarkModule(
        "æ•°æ®å¯¼å‡º", self.benchmark_data_export, "æµ‹è¯•æ•°æ®å¯¼å‡ºåˆ°æ–‡ä»¶æ€§èƒ½"
      ),
    ]

    # 3. è¿è¡Œæ‰€æœ‰æµ‹è¯•æ¨¡å—
    results = []

    # é¦–å…ˆæ·»åŠ XMLè§£æç»“æœï¼ˆå·²ç»åœ¨load_sample_dataä¸­å®Œæˆï¼‰
    xml_result = BenchmarkResult("XML è§£æ")
    xml_result.status = "completed"
    xml_result.time_seconds = xml_parse_metrics["parse_time_seconds"]
    xml_result.memory_mb = xml_parse_metrics["memory_delta_mb"]
    xml_result.records_processed = xml_parse_metrics["records_processed"]
    xml_result.throughput_records_per_sec = xml_parse_metrics[
      "throughput_records_per_sec"
    ]
    results.append(xml_result)

    # ç„¶åè¿è¡Œå…¶ä»–æµ‹è¯•æ¨¡å—
    for module in modules:
      result = self.run_module_with_timeout(module, sample_records)
      results.append(result)

    self.results = results

    # 4. è®¡ç®—æ€»ä½“ç»Ÿè®¡
    completed_count = sum(1 for r in results if r.status == "completed")
    total_time = time.time() - self.start_time

    logger.info("=== æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ ===")
    logger.info(f"æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f} ç§’")
    logger.info(f"æ ·æœ¬æ•°æ®é‡: {len(sample_records)} æ¡è®°å½•")
    logger.info(f"å®Œæˆæ¨¡å—: {completed_count}/{len(results)}")

    return results

  def print_report(self):
    """æ‰“å°æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
    if not self.results:
      logger.error("æ²¡æœ‰å¯ç”¨çš„åŸºå‡†æµ‹è¯•ç»“æœï¼Œè¯·å…ˆè¿è¡Œ run_all_benchmarks()")
      return

    console = Console()
    total_time = time.time() - self.start_time
    completed_count = sum(1 for r in self.results if r.status == "completed")

    # ä¸»æ ‡é¢˜é¢æ¿
    title = Text(
      "ğŸ Apple Health Analyzer - æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š", style="bold blue"
    )
    console.print(Panel(title, border_style="blue", padding=(1, 2)))

    # åŸºæœ¬ä¿¡æ¯è¡¨æ ¼
    info_table = Table(show_header=True, header_style="bold cyan", box=None)
    info_table.add_column("æŒ‡æ ‡", style="dim", width=10)
    info_table.add_column("æ•°å€¼", style="green")

    info_table.add_row("æµ‹è¯•å¼€å§‹æ—¶é—´", time.strftime("%Y-%m-%d %H:%M:%S"))
    info_table.add_row("æ€»æµ‹è¯•æ—¶é—´", f"{total_time:.2f} ç§’")
    info_table.add_row("æ ·æœ¬æ•°æ®é‡", f"{len(self.sample_records):,} æ¡è®°å½•")
    info_table.add_row("è¶…æ—¶è®¾ç½®", f"{self.timeout_seconds} ç§’")
    info_table.add_row("å®Œæˆæ¨¡å—", f"{completed_count}/{len(self.results)}")

    console.print(info_table)
    console.print()

    # æ€§èƒ½æŒ‡æ ‡è¡¨æ ¼
    perf_table = Table(
      title="ğŸ” å„æ¨¡å—æ€§èƒ½æŒ‡æ ‡",
      title_style="bold yellow",
      show_header=True,
      header_style="bold magenta",
      border_style="blue",
    )

    perf_table.add_column("æ¨¡å—åç§°", style="cyan", min_width=12)
    perf_table.add_column("çŠ¶æ€", style="green", min_width=6, justify="center")
    perf_table.add_column(
      "æ—¶é—´(ç§’)", style="yellow", min_width=10, justify="right"
    )
    perf_table.add_column("è®°å½•æ•°", style="blue", min_width=10, justify="right")
    perf_table.add_column(
      "ååé‡(æ¡/ç§’)", style="red", min_width=15, justify="right"
    )
    perf_table.add_column(
      "å†…å­˜å¢é‡(MB)", style="purple", min_width=12, justify="right"
    )

    for result in self.results:
      # çŠ¶æ€å›¾æ ‡å’Œé¢œè‰²
      status_config = {
        "completed": ("âœ…", "green"),
        "timeout": ("â°", "yellow"),
        "error": ("âŒ", "red"),
        "pending": ("â³", "blue"),
      }
      status_icon, status_color = status_config.get(
        result.status, ("â“", "white")
      )

      # æ ¼å¼åŒ–ååé‡ï¼Œé¿å…æ˜¾ç¤ºå¼‚å¸¸å¤§çš„æ•°å­—
      throughput = result.throughput_records_per_sec
      if throughput > 1000000:  # å¦‚æœååé‡è¶…è¿‡100ä¸‡ï¼Œæ˜¾ç¤ºä¸º"ç¬æ—¶"
        throughput_str = Text("ç¬æ—¶", style="bold green")
      else:
        throughput_str = f"{throughput:,.0f}"

      # å†…å­˜å¢é‡é¢œè‰²ï¼ˆæ­£æ•°çº¢è‰²è¡¨ç¤ºå¢åŠ ï¼Œè´Ÿæ•°ç»¿è‰²è¡¨ç¤ºå‡å°‘ï¼‰
      memory_color = "red" if result.memory_mb > 0 else "green"
      memory_str = f"{result.memory_mb:+.2f}"

      perf_table.add_row(
        result.module_name,
        Text(status_icon, style=status_color),
        f"{result.time_seconds:.2f}",
        f"{result.records_processed:,}",
        throughput_str,
        Text(memory_str, style=memory_color),
      )

    console.print(perf_table)

    # æ€§èƒ½ç“¶é¢ˆåˆ†æ
    if completed_count > 0:
      console.print("\nğŸ’¡ [bold cyan]æ€§èƒ½ç“¶é¢ˆåˆ†æ:[/bold cyan]")

      # æ‰¾å‡ºè€—æ—¶æœ€é•¿çš„æ¨¡å—
      sorted_by_time = sorted(
        [r for r in self.results if r.status == "completed"],
        key=lambda x: x.time_seconds,
        reverse=True,
      )
      if sorted_by_time:
        slowest = sorted_by_time[0]
        console.print(
          f"  âš ï¸  [red]{slowest.module_name}[/red]æ¨¡å—è€—æ—¶æœ€é•¿ï¼ˆ[bold]{slowest.time_seconds:.2f}ç§’[/bold]ï¼‰"
        )

      # æ‰¾å‡ºååé‡æœ€ä½çš„æ¨¡å—
      sorted_by_throughput = sorted(
        [r for r in self.results if r.status == "completed"],
        key=lambda x: x.throughput_records_per_sec,
      )
      if (
        sorted_by_throughput
        and sorted_by_throughput[0].throughput_records_per_sec > 0
      ):
        lowest_throughput = sorted_by_throughput[0]
        console.print(
          f"  âš ï¸  [red]{lowest_throughput.module_name}[/red]æ¨¡å—ååé‡æœ€ä½ï¼ˆ[bold]{lowest_throughput.throughput_records_per_sec:,.0f}æ¡/ç§’[/bold]ï¼‰"
        )

      # æ‰¾å‡ºå†…å­˜å ç”¨æœ€é«˜çš„æ¨¡å—
      sorted_by_memory = sorted(
        [r for r in self.results if r.status == "completed"],
        key=lambda x: x.memory_mb,
        reverse=True,
      )
      if sorted_by_memory and sorted_by_memory[0].memory_mb > 10:
        highest_memory = sorted_by_memory[0]
        memory_color = "red" if highest_memory.memory_mb > 0 else "green"
        console.print(
          f"  âš ï¸  [red]{highest_memory.module_name}[/red]æ¨¡å—å†…å­˜å ç”¨æœ€é«˜ï¼ˆ[bold {memory_color}]{highest_memory.memory_mb:.1f} MB[/bold {memory_color}]ï¼‰"
        )

    # å®Œæˆæ—¶é—´
    console.print(
      f"\nâœ… [green]æµ‹è¯•å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}[/green]"
    )


def run_benchmark(
  xml_path: str, output_dir: str | None = None, timeout: int = 30
):
  """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•çš„ä¾¿æ·å‡½æ•°"""
  xml_path_obj = Path(xml_path)
  if not xml_path_obj.exists():
    raise FileNotFoundError(f"XMLæ–‡ä»¶ä¸å­˜åœ¨: {xml_path}")

  output_dir_obj = Path(output_dir) if output_dir else None

  runner = BenchmarkRunner(xml_path_obj, output_dir_obj, timeout)
  results = runner.run_all_benchmarks()
  runner.print_report()

  return results
