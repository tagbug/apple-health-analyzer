"""Data cleaning and preprocessing module.

Provides data deduplication, merging, quality validation, and other functions.
"""

from collections import defaultdict
from datetime import datetime, timedelta
from typing import Any, cast

import pandas as pd
from pydantic import BaseModel, Field

from src.core.data_models import HealthRecord
from src.utils.logger import get_logger

logger = get_logger(__name__)


class RecordRowData(BaseModel):
  """DataFrame è¡Œæ•°æ®çš„ç±»å‹å®‰å…¨åŒ…è£…

  ç”¨äºåœ¨ DataFrame å’Œ HealthRecord å¯¹è±¡ä¹‹é—´è¿›è¡Œç±»å‹å®‰å…¨çš„è½¬æ¢ã€‚
  """

  type: str
  source_name: str
  start_date: datetime
  end_date: datetime | None
  creation_date: datetime
  source_version: str = "1.0"
  device: str = "Unknown"
  unit: str | None = None
  value: float | str | None = None
  metadata: dict[str, Any] | None = None

  @classmethod
  def from_series(cls, row: pd.Series, record_type: str) -> "RecordRowData":
    """ä» pandas Series åˆ›å»ºç±»å‹å®‰å…¨çš„æ•°æ®å¯¹è±¡"""
    # æ‰‹åŠ¨æå–å¹¶è½¬æ¢å­—æ®µï¼Œé¿å… pandas ç±»å‹æ¨æ–­é—®é¢˜
    start_date_val = row["start_date"]
    if isinstance(start_date_val, str):
      start_date = pd.to_datetime(start_date_val).to_pydatetime()
    elif isinstance(start_date_val, pd.Timestamp):
      start_date = start_date_val.to_pydatetime()
    else:
      # ä½¿ç”¨ cast æ˜ç¡®å‘Šè¯‰ç±»å‹æ£€æŸ¥å™¨è¿™æ˜¯ datetime
      start_date = cast(datetime, start_date_val)

    end_date_val = row.get("end_date")
    if end_date_val is not None and str(end_date_val).lower() not in (
      "",
      "nan",
      "none",
    ):
      if isinstance(end_date_val, str):
        end_date = pd.to_datetime(end_date_val).to_pydatetime()
      elif isinstance(end_date_val, pd.Timestamp):
        end_date = end_date_val.to_pydatetime()
      else:
        end_date = cast(datetime, end_date_val)
    else:
      end_date = None

    creation_date_val = row["creation_date"]
    if isinstance(creation_date_val, str):
      creation_date = pd.to_datetime(creation_date_val).to_pydatetime()
    elif isinstance(creation_date_val, pd.Timestamp):
      creation_date = creation_date_val.to_pydatetime()
    else:
      # ä½¿ç”¨ cast æ˜ç¡®å‘Šè¯‰ç±»å‹æ£€æŸ¥å™¨è¿™æ˜¯ datetime
      creation_date = cast(datetime, creation_date_val)

    # å®‰å…¨åœ°æå–å…¶ä»–å­—æ®µ
    source_name = str(row.get("source_name", "Unknown"))
    source_version = str(row.get("source_version", "1.0"))
    device = str(row.get("device", "Unknown"))

    unit_val = row.get("unit")
    unit = (
      str(unit_val)
      if unit_val is not None and str(unit_val).lower() not in ("", "nan", "none")
      else None
    )

    value_val = row.get("value")
    # å¯¹äºç¡çœ è®°å½•ç­‰åˆ†ç±»è®°å½•ï¼Œvalueæ˜¯å­—ç¬¦ä¸²ï¼›å¯¹äºæ•°é‡è®°å½•ï¼Œvalueæ˜¯æ•°å­—
    if value_val is not None and str(value_val).lower() not in (
      "",
      "nan",
      "none",
    ):
      # æ£€æŸ¥æ˜¯å¦æ˜¯ç¡çœ è®°å½•ç±»å‹
      record_type = row.get("type", "")
      if "SleepAnalysis" in record_type or "Category" in record_type:
        # åˆ†ç±»è®°å½•ä¿æŒå­—ç¬¦ä¸²
        value = str(value_val)
      else:
        # æ•°é‡è®°å½•è½¬æ¢ä¸ºfloat
        try:
          value = float(value_val)
        except (ValueError, TypeError):
          value = str(value_val)  # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¿æŒå­—ç¬¦ä¸²
    else:
      value = None

    metadata = row.get("metadata", {})
    if metadata is None:
      metadata = {}

    return cls(
      type=record_type,
      source_name=source_name,
      start_date=start_date,
      end_date=end_date,
      creation_date=creation_date,
      source_version=source_version,
      device=device,
      unit=unit,
      value=value,
      metadata=metadata,
    )

  def to_health_record(self) -> HealthRecord:
    """è½¬æ¢ä¸º HealthRecord"""
    # å¦‚æœ end_date ä¸º Noneï¼Œä½¿ç”¨ start_date ä½œä¸ºé»˜è®¤å€¼
    end_date = self.end_date if self.end_date is not None else self.start_date

    if self.value is not None:
      # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†ç±»è®°å½•ï¼ˆvalueä¸ºå­—ç¬¦ä¸²ï¼‰
      if isinstance(self.value, str):
        # åˆ›å»º CategoryRecord
        from src.core.data_models import CategoryRecord

        return CategoryRecord(
          type=self.type,
          source_name=self.source_name,
          start_date=self.start_date,
          end_date=end_date,  # ç°åœ¨ä¿è¯ä¸ä¸º None
          creation_date=self.creation_date,
          source_version=self.source_version,
          device=self.device,
          unit=None,  # åˆ†ç±»è®°å½•æ²¡æœ‰å•ä½
          value=self.value,
          metadata=self.metadata,
        )
      else:
        # åˆ›å»º QuantityRecordï¼ˆvalueä¸ºæ•°å­—ï¼‰
        from src.core.data_models import QuantityRecord

        return QuantityRecord(
          type=self.type,
          source_name=self.source_name,
          start_date=self.start_date,
          end_date=end_date,  # ç°åœ¨ä¿è¯ä¸ä¸º None
          creation_date=self.creation_date,
          source_version=self.source_version,
          device=self.device,
          unit=self.unit,
          value=self.value,
          metadata=self.metadata,
        )
    else:
      # åˆ›å»ºåŸºç¡€ HealthRecord
      return HealthRecord(
        type=self.type,
        source_name=self.source_name,
        start_date=self.start_date,
        end_date=end_date,  # ç°åœ¨ä¿è¯ä¸ä¸º None
        creation_date=self.creation_date,
        source_version=self.source_version,
        device=self.device,
        unit=self.unit,
        metadata=self.metadata,
      )


class DataQualityReport(BaseModel):
  """æ•°æ®è´¨é‡æŠ¥å‘Š"""

  total_records: int
  valid_records: int
  invalid_records: int
  duplicate_records: int
  cleaned_records: int
  quality_score: float  # 0-100

  # è¯¦ç»†ç»Ÿè®¡
  timestamp_issues: int = 0
  value_issues: int = 0
  metadata_issues: int = 0

  # æ•°æ®åˆ†å¸ƒ
  source_distribution: dict[str, int] = Field(default_factory=dict)
  type_distribution: dict[str, int] = Field(default_factory=dict)

  # æ—¶é—´èŒƒå›´
  date_range: dict[str, datetime | None] = Field(
    default_factory=lambda: {"start": None, "end": None}
  )


class DeduplicationResult(BaseModel):
  """å»é‡ç»“æœ"""

  original_count: int
  deduplicated_count: int
  removed_duplicates: int
  strategy_used: str
  processing_time_seconds: float

  # è¯¦ç»†ç»Ÿè®¡
  duplicates_by_source: dict[str, int] = Field(default_factory=dict)
  time_windows_processed: int = 0


class DataCleaner:
  """æ•°æ®æ¸…æ´—æ ¸å¿ƒç±»

  æä¾›å¤šç§æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†åŠŸèƒ½ï¼š
  - æ—¶é—´çª—å£å»é‡
  - æ•°æ®æºä¼˜å…ˆçº§å¤„ç†
  - å åŠ æ•°æ®åˆå¹¶
  - æ•°æ®è´¨é‡éªŒè¯
  """

  def __init__(
    self,
    source_priority: dict[str, int] | None = None,
    default_window_seconds: int = 60,
  ):
    """
    åˆå§‹åŒ–æ•°æ®æ¸…æ´—å™¨

    Args:
        source_priority: æ•°æ®æºä¼˜å…ˆçº§æ˜ å°„ï¼Œè¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
            ä¾‹å¦‚: {"ğŸ™Watch": 1, "å°ç±³è¿åŠ¨å¥åº·": 2, "ğŸ™Phone": 3}
        default_window_seconds: é»˜è®¤æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
    """
    # é»˜è®¤æ•°æ®æºä¼˜å…ˆçº§ï¼ˆæ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼‰
    self.source_priority = source_priority or {
      "ğŸ™Watch": 1,  # Apple Watch æœ€é«˜ä¼˜å…ˆçº§
      "Apple Watch": 1,  # åˆ«å
      "å°ç±³è¿åŠ¨å¥åº·": 2,  # å°ç±³è¿åŠ¨å¥åº·
      "Xiaomi Home": 2,  # åˆ«å
      "ğŸ™Phone": 3,  # iPhone æœ€ä½ä¼˜å…ˆçº§
      "iPhone": 3,  # åˆ«å
    }

    self.default_window_seconds = default_window_seconds
    logger.info(
      f"DataCleaner initialized with {len(self.source_priority)} source priorities"
    )

  def deduplicate_by_time_window(
    self,
    records: list[HealthRecord],
    window_seconds: int | None = None,
    strategy: str = "priority",
  ) -> tuple[list[HealthRecord], DeduplicationResult]:
    """
    åŸºäºæ—¶é—´çª—å£çš„å»é‡å¤„ç† (ä¼˜åŒ–ç‰ˆ)

    Args:
        records: å¾…å¤„ç†çš„è®°å½•åˆ—è¡¨
        window_seconds: æ—¶é—´çª—å£å¤§å°ï¼ˆç§’ï¼‰ï¼ŒNone ä½¿ç”¨é»˜è®¤å€¼
        strategy: å»é‡ç­–ç•¥
            - "priority": æŒ‰æ•°æ®æºä¼˜å…ˆçº§ä¿ç•™
            - "latest": ä¿ç•™æœ€æ–°çš„è®°å½•
            - "average": è®¡ç®—å¹³å‡å€¼ï¼ˆä»…æ•°å€¼ç±»å‹ï¼‰
            - "highest_quality": åŸºäºè´¨é‡è¯„åˆ†ä¿ç•™

    Returns:
        (å»é‡åçš„è®°å½•åˆ—è¡¨, å»é‡ç»“æœç»Ÿè®¡)
    """
    if not records:
      return [], DeduplicationResult(
        original_count=0,
        deduplicated_count=0,
        removed_duplicates=0,
        strategy_used=strategy,
        processing_time_seconds=0.0,
      )

    start_time = datetime.now()
    window = window_seconds or self.default_window_seconds

    logger.info(
      f"Starting optimized deduplication with strategy '{strategy}', window {window}s"
    )

    # æŒ‰è®°å½•ç±»å‹åˆ†ç»„å¤„ç†
    records_by_type = defaultdict(list)
    for record in records:
      records_by_type[record.type].append(record)

    deduplicated_records = []
    total_duplicates_removed = 0
    duplicates_by_source = defaultdict(int)

    for record_type, type_records in records_by_type.items():
      logger.debug(f"Processing {len(type_records)} records of type {record_type}")

      # è½¬æ¢ä¸º DataFrame ä¾¿äºå¤„ç†
      df = self._records_to_dataframe(type_records)

      # ç¡®ä¿ start_date æ˜¯ datetime ç±»å‹
      df["start_date"] = pd.to_datetime(df["start_date"])

      # ç¡®ä¿ creation_date æ˜¯ datetime ç±»å‹
      if "creation_date" in df.columns:
        df["creation_date"] = pd.to_datetime(df["creation_date"])

      # è®¡ç®—æ—¶é—´çª—å£
      # ä½¿ç”¨ floor å°†æ—¶é—´å‘ä¸‹å–æ•´åˆ°æœ€è¿‘çš„çª—å£
      df["time_window"] = df["start_date"].dt.floor(f"{window}s")

      original_count = len(df)

      if strategy == "priority":
        # è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•° (è¶Šå°è¶Šé«˜)
        # å°†æœªçŸ¥çš„æºè®¾ä¸ºæœ€ä½ä¼˜å…ˆçº§ (999)
        df["priority_score"] = df["source_name"].map(self.source_priority).fillna(999)

        # æŒ‰æ—¶é—´çª—å£å’Œä¼˜å…ˆçº§æ’åº (æ—¶é—´çª—å£å‡åº, ä¼˜å…ˆçº§å‡åº)
        df.sort_values(
          by=["time_window", "priority_score"], ascending=[True, True], inplace=True
        )

        # å»é‡ï¼Œä¿ç•™æ¯ä¸ªæ—¶é—´çª—å£çš„ç¬¬ä¸€æ¡è®°å½• (å³ä¼˜å…ˆçº§æœ€é«˜çš„)
        deduped_df = df.drop_duplicates(subset=["time_window"], keep="first")

      elif strategy == "latest":
        # æŒ‰æ—¶é—´çª—å£å’Œåˆ›å»ºæ—¶é—´æ’åº (æ—¶é—´çª—å£å‡åº, åˆ›å»ºæ—¶é—´é™åº)
        df.sort_values(
          by=["time_window", "creation_date"], ascending=[True, False], inplace=True
        )

        # å»é‡ï¼Œä¿ç•™æ¯ä¸ªæ—¶é—´çª—å£çš„ç¬¬ä¸€æ¡è®°å½• (å³æœ€æ–°çš„)
        deduped_df = df.drop_duplicates(subset=["time_window"], keep="first")

      elif strategy == "average" and self._is_numeric_type(record_type):
        # ç¡®ä¿ value åˆ—æ˜¯æ•°å€¼ç±»å‹
        df["value"] = pd.to_numeric(df["value"], errors="coerce")

        # æŒ‰æ—¶é—´çª—å£åˆ†ç»„è®¡ç®—å¹³å‡å€¼
        # æ³¨æ„: è¿™ä¼šä¸¢å¤±éèšåˆåˆ—çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬éœ€è¦ä¿ç•™å…ƒæ•°æ®ç­‰
        # è¿™é‡Œæˆ‘ä»¬å–æ¯ç»„çš„ç¬¬ä¸€æ¡è®°å½•ä½œä¸ºåŸºç¡€ï¼Œç„¶åæ›´æ–° value

        # 1. è®¡ç®—å¹³å‡å€¼å’Œè®¡æ•°
        grouped = df.groupby("time_window")["value"]
        avg_values = grouped.mean()
        counts = grouped.size()

        # 2. è·å–æ¯ç»„çš„ç¬¬ä¸€æ¡è®°å½•ä½œä¸ºæ¨¡æ¿
        deduped_df = df.drop_duplicates(subset=["time_window"], keep="first").set_index(
          "time_window"
        )

        # 3. æ›´æ–° value å’Œæ·»åŠ è®¡æ•°
        deduped_df["value"] = avg_values
        deduped_df["_count"] = counts
        deduped_df = deduped_df.reset_index()

        # 4. æ›´æ–°å…ƒæ•°æ® (éœ€è¦éå†ï¼Œè¿™éƒ¨åˆ†å¯èƒ½è¾ƒæ…¢ï¼Œä½†æ¯”å®Œå…¨å¾ªç¯å¥½)
        # ä¸ºäº†æ€§èƒ½ï¼Œè¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼Œåªæ ‡è®°è¿™æ˜¯ä¸€ä¸ªå¹³å‡å€¼
        # å¦‚æœéœ€è¦ç²¾ç¡®çš„å…ƒæ•°æ®æ›´æ–°ï¼Œå¯ä»¥åœ¨ _dataframe_row_to_record ä¸­å¤„ç†

      elif strategy == "highest_quality":
        # è®¡ç®—è´¨é‡åˆ†æ•°
        # 1. æºä¼˜å…ˆçº§åˆ†æ•° (0-40)
        df["priority_score"] = df["source_name"].map(self.source_priority).fillna(999)
        df["quality_score"] = (40 - (df["priority_score"] - 1) * 10).clip(lower=0)

        # 2. æ—¶é—´æˆ³åˆç†æ€§ (0-30)
        time_diff = (df["creation_date"] - df["start_date"]).abs().dt.total_seconds()
        df.loc[time_diff < 86400, "quality_score"] += 30
        df.loc[(time_diff >= 86400) & (time_diff < 604800), "quality_score"] += 20

        # æ’åº: è´¨é‡åˆ†æ•°é™åº
        df.sort_values(
          by=["time_window", "quality_score"], ascending=[True, False], inplace=True
        )

        # å»é‡
        deduped_df = df.drop_duplicates(subset=["time_window"], keep="first")

      else:
        # é»˜è®¤ä½¿ç”¨ä¼˜å…ˆçº§ç­–ç•¥
        df["priority_score"] = df["source_name"].map(self.source_priority).fillna(999)
        df.sort_values(
          by=["time_window", "priority_score"], ascending=[True, True], inplace=True
        )
        deduped_df = df.drop_duplicates(subset=["time_window"], keep="first")

      # è®¡ç®—ç§»é™¤çš„é‡å¤é¡¹
      removed_count = original_count - len(deduped_df)
      total_duplicates_removed += removed_count

      # ç»Ÿè®¡ç§»é™¤çš„æº
      if removed_count > 0:
        removed_mask = ~df.index.isin(deduped_df.index)
        removed_sources = df.loc[removed_mask, "source_name"].value_counts()
        for source, count in removed_sources.items():
          duplicates_by_source[source] += count

      # å°†ç»“æœè½¬æ¢å› HealthRecord å¯¹è±¡
      for _, row in deduped_df.iterrows():
        record = self._dataframe_row_to_record(row, record_type)
        if record:
          if strategy == "average" and self._is_numeric_type(record_type):
            # ä¸ºå¹³å‡å€¼ç­–ç•¥æ·»åŠ å…ƒæ•°æ®æ ‡è®°
            if record.metadata is None:
              record.metadata = {}
            record.metadata["deduplication_method"] = "average"
            if "_count" in row:
              record.metadata["original_records_count"] = int(row["_count"])

          deduplicated_records.append(record)

    processing_time = (datetime.now() - start_time).total_seconds()

    result = DeduplicationResult(
      original_count=len(records),
      deduplicated_count=len(deduplicated_records),
      removed_duplicates=total_duplicates_removed,
      strategy_used=strategy,
      processing_time_seconds=processing_time,
      duplicates_by_source=dict(duplicates_by_source),
      time_windows_processed=len(deduplicated_records),
    )

    logger.info(
      f"Deduplication completed: {result.original_count} -> "
      f"{result.deduplicated_count} records "
      f"({result.removed_duplicates} duplicates removed)"
    )

    return deduplicated_records, result

  def merge_overlapping_records(
    self, records: list[HealthRecord], merge_threshold_seconds: int = 5
  ) -> list[HealthRecord]:
    """
    åˆå¹¶é‡å æˆ–ç›¸é‚»çš„è®°å½•

    ä¸»è¦ç”¨äºç¡çœ æ•°æ®å’Œè¿åŠ¨æ•°æ®ï¼Œè¿™äº›æ•°æ®å¯èƒ½è¢«åˆ†å‰²æˆå¤šä¸ªè¿ç»­çš„è®°å½•ã€‚

    Args:
        records: å¾…åˆå¹¶çš„è®°å½•åˆ—è¡¨
        merge_threshold_seconds: åˆå¹¶é˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œè®°å½•é—´éš”å°äºæ­¤å€¼åˆ™åˆå¹¶

    Returns:
        åˆå¹¶åçš„è®°å½•åˆ—è¡¨
    """
    if not records or len(records) <= 1:
      return records

    logger.info(f"Merging overlapping records, threshold: {merge_threshold_seconds}s")

    # æŒ‰è®°å½•ç±»å‹åˆ†ç»„
    records_by_type = defaultdict(list)
    for record in records:
      records_by_type[record.type].append(record)

    merged_records = []

    for record_type, type_records in records_by_type.items():
      if not self._should_merge_type(record_type):
        # è¯¥ç±»å‹ä¸éœ€è¦åˆå¹¶
        merged_records.extend(type_records)
        continue

      # æ’åºå¹¶åˆå¹¶
      sorted_records = sorted(type_records, key=lambda r: r.start_date)
      merged = self._merge_sorted_records(sorted_records, merge_threshold_seconds)
      merged_records.extend(merged)

    logger.info(f"Merge completed: {len(records)} -> {len(merged_records)} records")
    return merged_records

  def validate_data_quality(self, records: list[HealthRecord]) -> DataQualityReport:
    """
    éªŒè¯æ•°æ®è´¨é‡å¹¶ç”ŸæˆæŠ¥å‘Š

    Args:
        records: å¾…éªŒè¯çš„è®°å½•åˆ—è¡¨

    Returns:
        æ•°æ®è´¨é‡æŠ¥å‘Š
    """
    if not records:
      return DataQualityReport(
        total_records=0,
        valid_records=0,
        invalid_records=0,
        duplicate_records=0,
        cleaned_records=0,
        quality_score=0.0,
      )

    logger.info(f"Validating data quality for {len(records)} records")

    total_records = len(records)
    valid_records = 0
    invalid_records = 0

    # è¯¦ç»†ç»Ÿè®¡
    timestamp_issues = 0
    value_issues = 0
    metadata_issues = 0

    # åˆ†å¸ƒç»Ÿè®¡
    source_distribution = defaultdict(int)
    type_distribution = defaultdict(int)

    # æ—¶é—´èŒƒå›´
    dates = []

    for record in records:
      is_valid = True

      # æ£€æŸ¥æ—¶é—´æˆ³
      if not self._validate_timestamp(record):
        timestamp_issues += 1
        is_valid = False

      # æ£€æŸ¥æ•°å€¼
      if not self._validate_value(record):
        value_issues += 1
        is_valid = False

      # æ£€æŸ¥å…ƒæ•°æ®
      if not self._validate_metadata(record):
        metadata_issues += 1
        # å…ƒæ•°æ®é—®é¢˜ä¸å½±å“è®°å½•æœ‰æ•ˆæ€§ï¼Œåªè®°å½•ç»Ÿè®¡

      if is_valid:
        valid_records += 1
      else:
        invalid_records += 1

      # ç»Ÿè®¡åˆ†å¸ƒ
      source_distribution[record.source_name] += 1
      type_distribution[record.type] += 1

      # æ”¶é›†æ—¥æœŸ
      dates.append(record.start_date)

    # è®¡ç®—è´¨é‡è¯„åˆ†
    quality_score = self._calculate_quality_score(
      total_records, valid_records, timestamp_issues, value_issues
    )

    # æ—¶é—´èŒƒå›´
    date_range = {
      "start": min(dates) if dates else None,
      "end": max(dates) if dates else None,
    }

    # æ£€æµ‹é‡å¤ï¼ˆç®€å•æ£€æµ‹ï¼ŒåŸºäºæ—¶é—´å’Œå€¼å®Œå…¨ç›¸åŒï¼‰
    duplicate_records = self._detect_duplicates(records)

    report = DataQualityReport(
      total_records=total_records,
      valid_records=valid_records,
      invalid_records=invalid_records,
      duplicate_records=duplicate_records,
      cleaned_records=valid_records,  # å‡è®¾æ¸…ç†åä¿ç•™æœ‰æ•ˆè®°å½•
      quality_score=quality_score,
      timestamp_issues=timestamp_issues,
      value_issues=value_issues,
      metadata_issues=metadata_issues,
      source_distribution=dict(source_distribution),
      type_distribution=dict(type_distribution),
      date_range=date_range,
    )

    logger.info(
      f"Quality validation completed: {valid_records}/{total_records} valid "
      f"(score: {quality_score:.1f})"
    )

    return report

  def _records_to_dataframe(self, records: list[HealthRecord]) -> pd.DataFrame:
    """å°†è®°å½•åˆ—è¡¨è½¬æ¢ä¸º DataFrame"""
    data = []
    for record in records:
      row = {
        "id": id(record),  # ä½¿ç”¨å¯¹è±¡IDä½œä¸ºå”¯ä¸€æ ‡è¯†
        "type": record.type,
        "source_name": record.source_name,
        "start_date": record.start_date,
        "end_date": record.end_date,
        "creation_date": record.creation_date,
        "value": getattr(record, "value", None),
        "unit": getattr(record, "unit", None),
        "metadata": getattr(record, "metadata", None),
      }
      data.append(row)

    return pd.DataFrame(data)

  def _dataframe_row_to_record(
    self, row: pd.Series, record_type: str
  ) -> HealthRecord | None:
    """å°† DataFrame è¡Œè½¬æ¢å›è®°å½•å¯¹è±¡"""
    try:
      # ä½¿ç”¨ RecordRowData ä¸­é—´ç±»è¿›è¡Œç±»å‹å®‰å…¨çš„è½¬æ¢
      row_data = RecordRowData.from_series(row, record_type)
      return row_data.to_health_record()

    except Exception as e:
      logger.error(f"Failed to reconstruct record: {e}")
      return None

  def _should_merge_type(self, record_type: str) -> bool:
    """åˆ¤æ–­è®°å½•ç±»å‹æ˜¯å¦éœ€è¦åˆå¹¶"""
    # ç¡çœ è®°å½•å’ŒæŸäº›è¿åŠ¨è®°å½•éœ€è¦åˆå¹¶
    merge_types = {
      "HKCategoryTypeIdentifierSleepAnalysis",
      "HKWorkoutTypeIdentifier",  # è¿åŠ¨è®°å½•
    }
    return record_type in merge_types

  def _merge_sorted_records(
    self, records: list[HealthRecord], threshold_seconds: int
  ) -> list[HealthRecord]:
    """åˆå¹¶å·²æ’åºçš„è®°å½•åˆ—è¡¨"""
    if not records:
      return []

    merged = [records[0]]

    for current in records[1:]:
      last = merged[-1]

      # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶
      if self._can_merge_records(last, current, threshold_seconds):
        # åˆå¹¶è®°å½•
        merged[-1] = self._merge_two_records(last, current)
      else:
        # ä¸èƒ½åˆå¹¶ï¼Œæ·»åŠ ä¸ºæ–°è®°å½•
        merged.append(current)

    return merged

  def _can_merge_records(
    self, record1: HealthRecord, record2: HealthRecord, threshold_seconds: int
  ) -> bool:
    """åˆ¤æ–­ä¸¤æ¡è®°å½•æ˜¯å¦å¯ä»¥åˆå¹¶"""
    # æ—¶é—´ä¸Šè¿ç»­æˆ–è½»å¾®é‡å 
    time_gap = (record2.start_date - record1.end_date).total_seconds()
    return time_gap <= threshold_seconds

  def _merge_two_records(
    self, record1: HealthRecord, record2: HealthRecord
  ) -> HealthRecord:
    """åˆå¹¶ä¸¤æ¡è®°å½•"""
    # åˆå¹¶æ—¶é—´èŒƒå›´ï¼šä½¿ç”¨è¾ƒæ—©çš„å¼€å§‹æ—¶é—´å’Œè¾ƒæ™šçš„ç»“æŸæ—¶é—´
    merged_start = min(record1.start_date, record2.start_date)
    merged_end = max(record1.end_date, record2.end_date)

    # åˆå¹¶å€¼ï¼šå¦‚æœéƒ½æ˜¯æ•°å€¼ç±»å‹ï¼Œå–å¹³å‡å€¼ï¼›å¦åˆ™ä¿ç•™ç¬¬ä¸€ä¸ªå€¼
    merged_value = None
    if (
      hasattr(record1, "value")
      and hasattr(record2, "value")
      and record1.value is not None
      and record2.value is not None
    ):
      if isinstance(record1.value, (int, float)) and isinstance(
        record2.value, (int, float)
      ):
        merged_value = (record1.value + record2.value) / 2
      else:
        merged_value = record1.value  # å¯¹äºéæ•°å€¼ç±»å‹ï¼Œä¿ç•™ç¬¬ä¸€ä¸ª

    # åˆå¹¶å…ƒæ•°æ®
    merged_metadata = {}
    if record1.metadata:
      merged_metadata.update(record1.metadata)
    if record2.metadata:
      merged_metadata.update(record2.metadata)
    merged_metadata["merged_from"] = 2  # æ ‡è®°è¿™æ˜¯åˆå¹¶çš„ç»“æœ

    # åˆ›å»ºåˆå¹¶åçš„è®°å½•ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè®°å½•ä½œä¸ºæ¨¡æ¿
    if hasattr(record1, "value") and merged_value is not None:
      # æ•°å€¼è®°å½•
      from src.core.data_models import QuantityRecord

      return QuantityRecord(
        type=record1.type,
        source_name=record1.source_name,
        start_date=merged_start,
        end_date=merged_end,
        creation_date=min(record1.creation_date, record2.creation_date),
        source_version=record1.source_version,
        device=record1.device,
        unit=getattr(record1, "unit", None),
        value=merged_value,
        metadata=merged_metadata,
      )
    elif hasattr(record1, "value"):
      # åˆ†ç±»è®°å½•
      from src.core.data_models import CategoryRecord

      return CategoryRecord(
        type=record1.type,
        source_name=record1.source_name,
        start_date=merged_start,
        end_date=merged_end,
        creation_date=min(record1.creation_date, record2.creation_date),
        source_version=record1.source_version,
        device=record1.device,
        value=merged_value or record1.value,
        metadata=merged_metadata,
      )
    else:
      # åŸºç¡€è®°å½•
      return HealthRecord(
        type=record1.type,
        source_name=record1.source_name,
        start_date=merged_start,
        end_date=merged_end,
        creation_date=min(record1.creation_date, record2.creation_date),
        source_version=record1.source_version,
        device=record1.device,
        metadata=merged_metadata,
      )

  def _validate_timestamp(self, record: HealthRecord) -> bool:
    """éªŒè¯æ—¶é—´æˆ³æœ‰æ•ˆæ€§"""
    try:
      # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦å­˜åœ¨
      if not hasattr(record, "start_date") or not record.start_date:
        return False

      # æ£€æŸ¥æ—¶é—´æˆ³åˆç†æ€§ï¼ˆä¸èƒ½æ˜¯æœªæ¥å¤ªè¿œçš„æ—¥æœŸï¼‰
      now = datetime.now(record.start_date.tzinfo)
      if record.start_date > now + timedelta(days=1):
        return False

      # æ£€æŸ¥å¼€å§‹æ—¶é—´ä¸èƒ½æ™šäºç»“æŸæ—¶é—´
      if hasattr(record, "end_date") and record.end_date:
        if record.start_date > record.end_date:
          return False

      return True
    except Exception:
      return False

  def _validate_value(self, record: HealthRecord) -> bool:
    """éªŒè¯æ•°å€¼æœ‰æ•ˆæ€§"""
    try:
      # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹è®°å½•
      if not self._is_numeric_type(record.type):
        return True  # éæ•°å€¼ç±»å‹è®°å½•ä¸éœ€è¦æ•°å€¼éªŒè¯

      # æ£€æŸ¥æ˜¯å¦æœ‰ value å±æ€§
      if not hasattr(record, "value"):
        return False  # æ•°å€¼ç±»å‹è®°å½•å¿…é¡»æœ‰ value

      value = getattr(record, "value", None)
      if value is None:
        return False  # æ•°å€¼ç±»å‹è®°å½•çš„ value ä¸èƒ½ä¸º None

      # åŸºæœ¬æ•°å€¼æ£€æŸ¥
      if not isinstance(value, (int, float)):
        return False

      # é’ˆå¯¹ä¸åŒè®°å½•ç±»å‹çš„ç‰¹æ®Šæ£€æŸ¥
      if record.type == "HKQuantityTypeIdentifierHeartRate":
        # å¿ƒç‡åº”è¯¥åœ¨ 30-250 bpm ä¹‹é—´
        return 30 <= value <= 250
      elif record.type == "HKQuantityTypeIdentifierBodyMass":
        # ä½“é‡åº”è¯¥åœ¨ 20-300 kg ä¹‹é—´
        return 20 <= value <= 300

      # å…¶ä»–ç±»å‹ä½¿ç”¨é€šç”¨æ£€æŸ¥
      return abs(value) < 1e10  # é¿å…æç«¯å€¼

    except Exception:
      return False

  def _validate_metadata(self, record: HealthRecord) -> bool:
    """éªŒè¯å…ƒæ•°æ®æœ‰æ•ˆæ€§"""
    try:
      if not hasattr(record, "metadata"):
        return True

      metadata = record.metadata
      if metadata is None:
        return True

      # æ£€æŸ¥å…ƒæ•°æ®æ˜¯å¦ä¸ºå­—å…¸
      if not isinstance(metadata, dict):
        return False

      # æ£€æŸ¥æ˜¯å¦æœ‰åŸºæœ¬çš„å…ƒæ•°æ®å­—æ®µ
      # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„å…ƒæ•°æ®éªŒè¯é€»è¾‘

      return True
    except Exception:
      return False

  def _calculate_quality_score(
    self, total: int, valid: int, timestamp_issues: int, value_issues: int
  ) -> float:
    """è®¡ç®—è´¨é‡è¯„åˆ† (0-100)"""
    if total == 0:
      return 0.0

    # æœ‰æ•ˆæ€§è¯„åˆ† (60% æƒé‡)
    validity_score = (valid / total) * 60

    # é—®é¢˜ä¸¥é‡ç¨‹åº¦è¯„åˆ† (40% æƒé‡)
    issue_penalty = ((timestamp_issues + value_issues) / total) * 40

    return max(0.0, min(100.0, validity_score - issue_penalty))

  def _detect_duplicates(self, records: list[HealthRecord]) -> int:
    """ç®€å•é‡å¤æ£€æµ‹"""
    seen = set()
    duplicates = 0

    for record in records:
      # åˆ›å»ºè®°å½•çš„ç­¾åï¼ˆç±»å‹ + æ—¶é—´ + å€¼ï¼‰
      signature = (
        record.type,
        record.start_date.isoformat(),
        getattr(record, "value", None),
        record.source_name,
      )

      if signature in seen:
        duplicates += 1
      else:
        seen.add(signature)

    return duplicates

  def _is_numeric_type(self, record_type: str) -> bool:
    """åˆ¤æ–­è®°å½•ç±»å‹æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹"""
    numeric_types = {
      "HKQuantityTypeIdentifierHeartRate",
      "HKQuantityTypeIdentifierBodyMass",
      "HKQuantityTypeIdentifierBodyMassIndex",
      "HKQuantityTypeIdentifierHeight",
      "HKQuantityTypeIdentifierHeartRateVariabilitySDNN",
    }
    return record_type in numeric_types
